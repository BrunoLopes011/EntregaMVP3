{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6dab859",
      "metadata": {
        "id": "f6dab859"
      },
      "source": [
        "\n",
        "# Template — MVP: *Machine Learning & Analytics*\n",
        "**Autor:** Bruno dos Santos Lopes\n",
        "\n",
        "**Data:** 27/09/2025\n",
        "\n",
        "**Matrícula:** 4052025000271\n",
        "\n",
        "**Dataset:** [Store Sales - Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data)\n",
        "\n",
        "> **Importante:**\n",
        "A estrutura base deste notebook pode servir como um guia inicial para desenvolver suas análises, já que contempla grande parte das sugestões do checklist apresentado no enunciado do MVP. Entretanto, é importante destacar que esta estrutura é apenas um ponto de partida: poderão ser necessárias etapas e análises adicionais além das aqui exemplificadas.\n",
        "\n",
        "> O essencial é garantir profundidade nas discussões e análises, construindo um storytelling consistente que explore os principais conceitos e técnicas vistos nas aulas da Sprint de Machine Learning & Analytics.\n",
        "\n",
        "> Lembre-se: não existe uma receita pronta. A ordem e as seções detalhadas abaixo são apenas sugestões. O problema escolhido e a história que você deseja contar devem guiar, em grande parte, a forma final do seu trabalho.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e69fe5",
      "metadata": {
        "id": "e9e69fe5"
      },
      "source": [
        "\n",
        "## ✅ Checklist do MVP (o que precisa conter)\n",
        "- [ ] **Problema definido** e contexto de negócio\n",
        "- [ ] **Carga e preparação** dos dados (sem vazamento de dados)\n",
        "- [ ] **Divisão** em treino/validação/teste (ou validação cruzada apropriada)\n",
        "- [ ] **Tratamento**: limpeza, transformação e **engenharia de atributos**\n",
        "- [ ] **Modelagem**: comparar abordagens/modelos (com **baseline**)\n",
        "- [ ] **Otimização de hiperparâmetros**\n",
        "- [ ] **Avaliação** com **métricas adequadas** e discussão de limitações\n",
        "- [ ] **Boas práticas**: seeds fixas, tempo de treino, recursos computacionais, documentação\n",
        "- [ ] **Pipelines reprodutíveis** (sempre que possível)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a670b1e8",
      "metadata": {
        "id": "a670b1e8"
      },
      "source": [
        "\n",
        "## 1. Escopo, objetivo e definição do problema\n",
        "**TODO:** Explique brevemente:\n",
        "- Contexto do problema e objetivo:\n",
        "O problema central abordado neste MVP é a ruptura de estoque, situação em que a demanda de um produto não pode ser atendida devido à falta de reposição adequada. Isso gera perda de vendas, redução da satisfação do cliente e ineficiências na cadeia logística.\n",
        "O objetivo do MVP é desenvolver uma solução baseada em dados que permita prever a demanda e apoiar a reposição de produtos, reduzindo a probabilidade de ruptura.\n",
        "Empresas de varejo precisam prever a demanda de produtos em diferentes lojas para otimizar estoque, reduzir perdas e maximizar o faturamento. O dataset Store Sales – Time Series Forecasting, disponibilizado pelo Kaggle, contém dados históricos de vendas diárias por loja e família de produtos, incluindo informações de promoções, feriados e indicadores externos.\n",
        "O objetivo deste projeto é desenvolver modelos de previsão de vendas (forecasting) que permitam estimar a demanda futura por produto/loja, considerando tendências, sazonalidades e eventos externos.  \n",
        "\n",
        "- Tipo de tarefa: Problema de previsão de séries temporais (forecasting)\n",
        "- Natureza do problema: Regressão (variável-alvo = valor numérico de vendas).  \n",
        "- Área de aplicação: Dados tabulares e temporais e Aplicação em Analytics e Inteligência de Negócios (BI/AI para varejo).  \n",
        "- Gestão de estoque eficiente: evitar ruptura (falta de produtos) e excesso (desperdício e custos). Planejamento de promoções: prever impacto de descontos e campanhas sazonais. Alocação de recursos: otimizar logística, distribuição e compras junto a fornecedores. Suporte à tomada de decisão: fornecer previsões confiáveis para gestores de diferentes áreas (vendas, marketing, supply chain).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1bd6e05",
      "metadata": {
        "id": "e1bd6e05"
      },
      "source": [
        "\n",
        "## 2. Reprodutibilidade e ambiente\n",
        "Especifique o ambiente. Por exemplo:\n",
        "- Bibliotecas usadas.\n",
        "- Seeds fixas para reprodutibilidade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b5cec9",
      "metadata": {
        "id": "42b5cec9"
      },
      "outputs": [],
      "source": [
        "# === Setup básico e reprodutibilidade ===\n",
        "import os, random, sys, math, time, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "# Scikit-learn (pré-processamento, modelagem, avaliação)\n",
        "from sklearn.model_selection import (train_test_split, StratifiedKFold, KFold, TimeSeriesSplit, RandomizedSearchCV)\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score, confusion_matrix,\n",
        "    mean_absolute_error, mean_squared_error, r2_score,\n",
        "    silhouette_score\n",
        ")\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "# Modelagem de séries temporais\n",
        "import statsmodels.api as sm\n",
        "from prophet import Prophet\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Configurações globais\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "# Para frameworks adicionais:\n",
        "# import torch; torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "# import tensorflow as tf; tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"Seed global:\", SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Funções python (opcional)\n",
        "Defina, se necessário, funções em Python para reutilizar seu código e torná-lo mais organizado. Essa é uma boa prática de programação que facilita a leitura, manutenção e evolução do seu projeto."
      ],
      "metadata": {
        "id": "yTTTI_gjtEbp"
      },
      "id": "yTTTI_gjtEbp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para criar lags de séries temporais\n",
        "def create_lag_features(df, group_cols, target_col, lags=[1,7,14,28]):\n",
        "    \"\"\"\n",
        "    Cria colunas de lags para séries temporais agrupadas.\n",
        "\n",
        "    Parâmetros:\n",
        "    - df: DataFrame contendo a série temporal\n",
        "    - group_cols: colunas para agrupar (ex.: ['store_nbr', 'family'])\n",
        "    - target_col: coluna alvo (ex.: 'sales')\n",
        "    - lags: lista de períodos de lag\n",
        "\n",
        "    Retorno:\n",
        "    - DataFrame com novas colunas de lags\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    for lag in lags:\n",
        "        df_copy[f'{target_col}_lag_{lag}'] = df_copy.groupby(group_cols)[target_col].shift(lag)\n",
        "    return df_copy\n",
        "\n",
        "# Função para calcular métricas de previsão\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def evaluate_forecast(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calcula RMSE, MAE e MAPE de uma previsão.\n",
        "\n",
        "    Parâmetros:\n",
        "    - y_true: valores reais\n",
        "    - y_pred: valores previstos\n",
        "\n",
        "    Retorno:\n",
        "    - dicionário com métricas\n",
        "    \"\"\"\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = (abs(y_true - y_pred) / (y_true + 1e-9)).mean() * 100  # evita divisão por zero\n",
        "    return {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}\n",
        "\n",
        "    # Função para plotar série temporal\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_series(df, date_col, target_col, title=None, figsize=(12,5)):\n",
        "    \"\"\"\n",
        "    Plota série temporal.\n",
        "\n",
        "    Parâmetros:\n",
        "    - df: DataFrame\n",
        "    - date_col: coluna de datas\n",
        "    - target_col: coluna de valores\n",
        "    - title: título opcional\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.plot(df[date_col], df[target_col], marker='o', linestyle='-')\n",
        "    plt.xlabel('Data')\n",
        "    plt.ylabel(target_col)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6PQYS9MftOLB"
      },
      "id": "6PQYS9MftOLB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ad1d8aa8",
      "metadata": {
        "id": "ad1d8aa8"
      },
      "source": [
        "\n",
        "## 3. Dados: carga, entendimento e qualidade\n",
        "O presente projeto utiliza o dataset Store Sales — Time Series Forecasting, disponibilizado no Kaggle, cujo objetivo é prever a demanda diária de produtos em diferentes lojas, permitindo otimizar estoques, reduzir perdas e apoiar decisões estratégicas de marketing e logística. Os dados foram carregados diretamente do GitHub, garantindo a execução imediata do notebook no Google Colab, sem necessidade de configuração adicional.\n",
        "\n",
        "O dataset é composto por múltiplos arquivos CSV, cada um com informações complementares sobre vendas, lojas, feriados, eventos e preços de petróleo. O arquivo train.csv contém as vendas históricas diárias por loja e família de produtos, sendo a variável-alvo a coluna sales. O arquivo test.csv corresponde ao conjunto de teste, utilizado para submissão de previsões. O arquivo stores.csv fornece dados adicionais sobre as lojas, incluindo cidade, estado, tipo e cluster de classificação. O arquivo holidays_events.csv lista feriados nacionais e regionais, bem como eventos especiais, indicando se foram transferidos. O arquivo transactions.csv informa o número diário de transações por loja, e o arquivo oil.csv registra o preço diário do petróleo WTI, que pode impactar o comportamento de vendas de certos produtos. Por fim, o arquivo sample_submission.csv serve como template para submissão das previsões no Kaggle.\n",
        "\n",
        "As principais variáveis presentes no dataset incluem: store_nbr (número da loja), family (categoria do produto), date (data da venda), sales (quantidade vendida), city, state, type e cluster das lojas, além de variáveis relacionadas a feriados (type, locale, locale_name, transferred), número de transações diárias e preço do petróleo (dcoilwtico).\n",
        "\n",
        "Uma análise preliminar da qualidade dos dados indicou a presença de valores ausentes em algumas colunas, como o preço do petróleo, que será tratado via interpolação temporal. Zeros nas vendas podem indicar dias sem vendas ou feriados, e datas de feriados transferidos foram cuidadosamente consideradas para evitar inconsistências. Todas as transformações e criação de features de lag serão realizadas apenas com informações históricas, evitando vazamento de dados e garantindo a reprodutibilidade das previsões.\n",
        "\n",
        "Do ponto de vista ético, os dados são públicos, voltados para fins educacionais e não contêm informações pessoais ou sensíveis. O uso das informações é restrito à análise de séries temporais de vendas e à construção de modelos de previsão de demanda, respeitando a confidencialidade e a integridade dos dados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e03c90f7",
      "metadata": {
        "id": "e03c90f7"
      },
      "outputs": [],
      "source": [
        "#Carga dos dados\n",
        "# URL base dos arquivos CSV no GitHub\n",
        "base_url = \"https://raw.githubusercontent.com/BrunoLopes011/EntregaMVP3/main/\"\n",
        "\n",
        "# Lista de arquivos\n",
        "arquivos = [\n",
        "    \"holidays_events.csv\",\n",
        "    \"oil.csv\",\n",
        "    \"sample_submission.csv\",\n",
        "    \"stores.csv\",\n",
        "    \"test.csv\",\n",
        "    \"train.csv\",\n",
        "    \"transactions.csv\"\n",
        "]\n",
        "\n",
        "# Leitura dos arquivos com pandas\n",
        "print(\"📥 Lendo arquivos diretamente do GitHub...\")\n",
        "holidays_events = pd.read_csv(base_url + \"holidays_events.csv\")\n",
        "oil = pd.read_csv(base_url + \"oil.csv\")\n",
        "sample_submission = pd.read_csv(base_url + \"sample_submission.csv\")\n",
        "stores = pd.read_csv(base_url + \"stores.csv\")\n",
        "test = pd.read_csv(base_url + \"test.csv\")\n",
        "train = pd.read_csv(base_url + \"train.csv\")\n",
        "transactions = pd.read_csv(base_url + \"transactions.csv\")\n",
        "\n",
        "print(\"✅ Arquivos carregados com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints dos cabeçalhos dos arquivos\n",
        "\n",
        "print(\"🏬 Vendas históricas (train)\")\n",
        "display(train.head())\n",
        "\n",
        "print(\"🧪 Conjunto de teste (test)\")\n",
        "display(test.head())\n",
        "\n",
        "print(\"🏪 Informações das lojas (stores)\")\n",
        "display(stores.head())\n",
        "\n",
        "print(\"📅 Feriados e eventos (holidays_events)\")\n",
        "display(holidays_events.head())\n",
        "\n",
        "print(\"💰 Preço do petróleo (oil)\")\n",
        "display(oil.head())\n",
        "\n",
        "print(\"💳 Número de transações por loja (transactions)\")\n",
        "display(transactions.head())\n",
        "\n",
        "print(\"📄 Template de submissão (sample_submission)\")\n",
        "display(sample_submission.head())\n",
        "\n",
        "\n",
        "# Total de Instâncias e Atributos por Arquivo\n",
        "\n",
        "# Dicionário com os datasets carregados\n",
        "datasets = {\n",
        "    \"train.csv\": train,\n",
        "    \"test.csv\": test,\n",
        "    \"stores.csv\": stores,\n",
        "    \"holidays_events.csv\": holidays_events,\n",
        "    \"transactions.csv\": transactions,\n",
        "    \"oil.csv\": oil,\n",
        "    \"sample_submission.csv\": sample_submission\n",
        "}\n",
        "\n",
        "# Função para exibir tamanho dos datasets\n",
        "print(\"📊 Total de Instâncias e Atributos por Arquivo:\\n\")\n",
        "\n",
        "for nome, df in datasets.items():\n",
        "    print(f\"📁 {nome}:\")\n",
        "    print(f\"   🔢 {df.shape[0]:,} registros\")\n",
        "    print(f\"   📐 {df.shape[1]} colunas\\n\")\n"
      ],
      "metadata": {
        "id": "yeaiTenpV3Vu"
      },
      "id": "yeaiTenpV3Vu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divisão dos dados\n",
        "\n",
        "A divisão dos dados foi realizada em treino (70%) e teste (30%).\n",
        "Embora a validação cruzada (k-fold) seja uma técnica recomendada para reduzir a variância das estimativas, neste projeto optamos por não utilizá-la devido ao grande volume de dados do Instacart, que tornaria a execução inviável computacionalmente dentro do escopo do MVP. Ainda assim, mantivemos uma amostra de validação para evitar data leakage e garantir avaliação robusta."
      ],
      "metadata": {
        "id": "zN5ylVoYcwxW"
      },
      "id": "zN5ylVoYcwxW"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Selecionar colunas que realmente existem\n",
        "wanted_cols = ['onpromotion', 'store_nbr', 'family']\n",
        "feature_cols = [c for c in wanted_cols if c in df.columns]\n",
        "\n",
        "# Definir X e y\n",
        "X = df[feature_cols].copy()\n",
        "y = df['sales'].copy()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Separar features numéricas e categóricas existentes\n",
        "numeric_features = [c for c in ['onpromotion'] if c in X_train.columns]\n",
        "categorical_features = [c for c in ['store_nbr', 'family'] if c in X_train.columns]\n",
        "\n",
        "# Filtrar X_train e X_test para apenas essas colunas (mantendo DataFrame)\n",
        "X_train = X_train[numeric_features + categorical_features].copy()\n",
        "X_test  = X_test[numeric_features + categorical_features].copy()\n",
        "\n",
        "numeric_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"num\", numeric_pipe, numeric_features),\n",
        "    (\"cat\", categorical_pipe, categorical_features)\n",
        "])\n",
        "\n",
        "print(\"✅ ColumnTransformer pronto.\")\n",
        "print(\"Colunas numéricas:\", numeric_features)\n",
        "print(\"Colunas categóricas:\", categorical_features)\n",
        "\n"
      ],
      "metadata": {
        "id": "nF21lhtoc46t"
      },
      "id": "nF21lhtoc46t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a0bb41",
      "metadata": {
        "id": "c2a0bb41"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Verificações iniciais ===\n",
        "def explorar_dataset(df, nome_arquivo):\n",
        "    print(f\"📂 Explorando: {nome_arquivo}\\n\")\n",
        "\n",
        "    print(\"🔹 Amostra aleatória de 5 registros:\")\n",
        "    display(df.sample(5))\n",
        "\n",
        "    print(\"\\n🔹 Formato do dataset:\")\n",
        "    print(df.shape)\n",
        "\n",
        "    print(\"\\n🔹 Tipos de dados das colunas:\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    print(\"\\n🔹 Valores ausentes por coluna:\")\n",
        "    print(df.isna().sum())\n",
        "\n",
        "# Exemplo de uso:\n",
        "explorar_dataset(train, \"train.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tratamento de dados\n",
        "\n",
        "Foi aplicada padronização nos atributos numéricos, de modo a evitar que variáveis em escalas diferentes prejudiquem o desempenho dos modelos (principalmente Regressão Logística). Além disso, mantivemos versões transformadas do dataset para posterior avaliação."
      ],
      "metadata": {
        "id": "b5_WfAuGdLWs"
      },
      "id": "b5_WfAuGdLWs"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Identificar colunas numéricas e categóricas\n",
        "num_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "cat_features = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Criar transformador\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Aplicar transformação\n",
        "X_train_scaled = preprocessor.fit_transform(X_train)\n",
        "X_test_scaled = preprocessor.transform(X_test)"
      ],
      "metadata": {
        "id": "DZ6fQ0YidOJ7"
      },
      "id": "DZ6fQ0YidOJ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelagem\n",
        "\n",
        "Para garantir reprodutibilidade e evitar retrabalho em diferentes etapas, utilizamos Pipeline do sklearn, que une pré-processamento e modelagem em um fluxo único. Isso facilita tanto o treino quanto a otimização de hiperparâmetros."
      ],
      "metadata": {
        "id": "dHIjXESDdhkP"
      },
      "id": "dHIjXESDdhkP"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pipeline Random Forest\n",
        "rf_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler(with_mean=False)),  # opcional para RF\n",
        "    ('rf', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Pipeline Regressão Logística\n",
        "lr_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n"
      ],
      "metadata": {
        "id": "cr-eaedCddrY"
      },
      "id": "cr-eaedCddrY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4I80l0EhddXv"
      },
      "id": "4I80l0EhddXv"
    },
    {
      "cell_type": "markdown",
      "id": "4922028c",
      "metadata": {
        "id": "4922028c"
      },
      "source": [
        "\n",
        "### 3.1 Análise exploratória resumida (EDA)\n",
        "A análise exploratória (EDA) tem como objetivo entender a distribuição das vendas, identificar padrões sazonais e correlacionar fatores externos que podem influenciar a demanda. Nesta seção, focamos em insights que impactam diretamente a modelagem de séries temporais e a criação de features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b93654e5",
      "metadata": {
        "id": "b93654e5"
      },
      "outputs": [],
      "source": [
        "# Total de vendas agregadas por dia\n",
        "vendas_diarias = train.groupby(\"date\")[\"sales\"].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(vendas_diarias[\"date\"], vendas_diarias[\"sales\"], marker='', linestyle='-')\n",
        "plt.title(\"Vendas Totais por Dia\")\n",
        "plt.xlabel(\"Data\")\n",
        "plt.ylabel(\"Vendas\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Vendas históricas\n",
        "# Boxplot por família de produtos (train)\n",
        "plt.figure(figsize=(16,6))\n",
        "train.boxplot(column=\"sales\", by=\"family\", rot=90)\n",
        "plt.title(\"Distribuição de Vendas por Família de Produtos\")\n",
        "plt.suptitle(\"\")\n",
        "plt.ylabel(\"Vendas\")\n",
        "plt.show()\n",
        "\n",
        "#Distribuição das vendas por família de produtos\n",
        "# Contagem de registros por loja\n",
        "train[\"store_nbr\"].value_counts().sort_index().plot(kind=\"bar\", figsize=(12,4))\n",
        "plt.title(\"Número de Registros por Loja\")\n",
        "plt.xlabel(\"Loja\")\n",
        "plt.ylabel(\"Quantidade de Dias\")\n",
        "plt.show()\n",
        "\n",
        "# Impacto de feriados e eventos\n",
        "# Vendas médias em feriados vs dias normais\n",
        "feriados = holidays_events[holidays_events[\"type\"].isin([\"Holiday\", \"Event\"])]\n",
        "dias_feriado = train[train[\"date\"].isin(feriados[\"date\"])]\n",
        "dias_normais = train[~train[\"date\"].isin(feriados[\"date\"])]\n",
        "\n",
        "print(\"Média de vendas em dias de feriado/evento:\", dias_feriado[\"sales\"].mean())\n",
        "print(\"Média de vendas em dias normais:\", dias_normais[\"sales\"].mean())\n",
        "\n",
        "#Transações x Vendas\n",
        "# Agrupar por dia e loja\n",
        "df_trans_vendas = train.merge(transactions, on=[\"date\",\"store_nbr\"], how=\"left\")\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.scatter(df_trans_vendas[\"transactions\"], df_trans_vendas[\"sales\"], alpha=0.3)\n",
        "plt.title(\"Relação: Transações x Vendas\")\n",
        "plt.xlabel(\"Transações\")\n",
        "plt.ylabel(\"Vendas\")\n",
        "plt.show()\n",
        "\n",
        "#Preço do petróleo x Vendas\n",
        "# Merge diário com média de vendas\n",
        "df_oil = train.groupby(\"date\")[\"sales\"].sum().reset_index().merge(oil, on=\"date\", how=\"left\")\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df_oil[\"date\"], df_oil[\"sales\"], label=\"Vendas Totais\")\n",
        "plt.plot(df_oil[\"date\"], df_oil[\"dcoilwtico\"]*1000, label=\"Preço do Petróleo (escala ajustada)\", color=\"orange\")\n",
        "plt.title(\"Vendas Totais vs Preço do Petróleo\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92d88464",
      "metadata": {
        "id": "92d88464"
      },
      "source": [
        "\n",
        "## 4. Definição do target, variáveis e divisão dos dados\n",
        "Para este projeto, a tarefa escolhida é previsão de séries temporais (forecasting), com o objetivo de estimar a quantidade de vendas diárias (sales) por loja e família de produtos. A variável-alvo definida é, portanto, sales, enquanto as features incluem quatro colunas relevantes derivadas dos dados originais, excluindo a coluna de data (date) para evitar vazamento de informações futuras.\n",
        "\n",
        "A divisão dos dados respeitou a ordem temporal, de modo a não embaralhar os registros, garantindo que o modelo utilize apenas informações passadas para prever o futuro. Foi realizado um corte temporal simples, com 80% dos registros para treino e 20% para teste, resultando em 438.208 observações de treino e 109.553 observações de teste. Essa abordagem preserva a sequência cronológica essencial em séries temporais.\n",
        "\n",
        "Além disso, foi aplicada uma validação cruzada temporal utilizando TimeSeriesSplit com 5 folds, permitindo avaliar a robustez do modelo em diferentes períodos históricos. Cada fold manteve a ordem temporal e foi estruturado da seguinte forma:\n",
        "\n",
        "- Fold 1: treino=7938 | validação=7935\n",
        "- Fold 2: treino=15873 | validação=7935\n",
        "- Fold 3: treino=23808 | validação=7935\n",
        "- Fold 4: treino=31743 | validação=7935\n",
        "- Fold 5: treino=39678 | validação=7935\n",
        "\n",
        "Todas as transformações aplicadas aos dados, como criação de features e normalizações, foram ajustadas apenas nos dados de treino e posteriormente aplicadas aos conjuntos de validação e teste, garantindo reprodutibilidade e ausência de vazamento de informações. Para maior organização e consistência, recomenda-se o uso de pipelines, integrando pré-processamento, engenharia de features e modelagem em um fluxo único.\n",
        "\n",
        "Essa configuração permite que o modelo aprenda padrões históricos, capture sazonalidade e tendências de vendas, e seja avaliado de forma realista em dados futuros, fornecendo previsões confiáveis para suporte à decisão em operações comerciais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5256d179",
      "metadata": {
        "id": "5256d179"
      },
      "outputs": [],
      "source": [
        "# === Definição do tipo de problema ===\n",
        "PROBLEM_TYPE = \"serie_temporal\"  # previsão de vendas\n",
        "\n",
        "# Target e features\n",
        "target = \"sales\"\n",
        "features = [c for c in train.columns if c not in [target, \"date\"]]\n",
        "\n",
        "print(\"PROBLEM_TYPE:\", PROBLEM_TYPE)\n",
        "print(\"Target:\", target)\n",
        "print(\"N features:\", len(features))\n",
        "\n",
        "# Ordenar dados por data\n",
        "train_sorted = train.sort_values(\"date\")\n",
        "\n",
        "# Corte temporal: 80% treino, 20% teste\n",
        "cutoff = int(len(train_sorted) * 0.8)\n",
        "train_ts = train_sorted.iloc[:cutoff]\n",
        "test_ts = train_sorted.iloc[cutoff:]\n",
        "\n",
        "X_train, y_train = train_ts[features], train_ts[target]\n",
        "X_test, y_test   = test_ts[features], test_ts[target]\n",
        "\n",
        "print(\"Treino:\", X_train.shape, \"| Teste:\", X_test.shape)\n",
        "\n",
        "# Exemplo de validação cruzada temporal\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n",
        "    print(f\"Fold {fold}: treino={len(train_idx)} | validação={len(val_idx)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a6a61e2",
      "metadata": {
        "id": "2a6a61e2"
      },
      "source": [
        "\n",
        "## 5. Tratamento de dados e **Pipeline** de pré-processamento\n",
        "Para garantir reprodutibilidade e evitar vazamento de dados, todos os passos de pré-processamento foram organizados em pipelines utilizando scikit-learn. O pipeline inclui as seguintes etapas:\n",
        "\n",
        "Limpeza e imputação: valores ausentes nas colunas numéricas são preenchidos com a mediana, enquanto valores ausentes em colunas categóricas são preenchidos com a moda (valor mais frequente).\n",
        "\n",
        "Escalonamento: colunas numéricas são padronizadas usando StandardScaler para melhorar a convergência de modelos baseados em gradiente e reduzir o impacto de diferentes magnitudes nas features.\n",
        "\n",
        "Encoding: colunas categóricas são convertidas em variáveis dummy via OneHotEncoder, garantindo que o modelo possa interpretar variáveis não numéricas sem perda de informação.\n",
        "\n",
        "Seleção de atributos (opcional): dependendo do modelo, podem ser aplicadas técnicas de feature selection após o pré-processamento.\n",
        "\n",
        "O uso de ColumnTransformer permite aplicar transformações distintas para colunas numéricas e categóricas dentro de um único pipeline, garantindo que as mesmas transformações aprendidas no treino sejam aplicadas no teste e validação, evitando vazamento de informação.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c6257ce",
      "metadata": {
        "id": "1c6257ce"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Identificação de colunas numéricas e categóricas\n",
        "num_cols = [c for c in X_train.columns if str(X_train[c].dtype).startswith((\"float\",\"int\"))]\n",
        "cat_cols = [c for c in X_train.columns if c not in num_cols and c != \"date\"]\n",
        "\n",
        "# Pipeline para colunas numéricas\n",
        "numeric_pipe = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline para colunas categóricas\n",
        "categorical_pipe = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "# Pipeline final combinando numéricas e categóricas\n",
        "preprocess = ColumnTransformer(transformers=[\n",
        "    (\"num\", numeric_pipe, num_cols),\n",
        "    (\"cat\", categorical_pipe, cat_cols)\n",
        "])\n",
        "\n",
        "print(\"Colunas numéricas:\", num_cols[:5], \"...\")\n",
        "print(\"Colunas categóricas:\", cat_cols[:5], \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a161e54",
      "metadata": {
        "id": "2a161e54"
      },
      "source": [
        "\n",
        "## 6. Baseline e modelos candidatos\n",
        "Para garantir uma referência inicial, iniciamos com uma baseline simples. Em séries temporais, uma abordagem comum é o modelo \"naive\", que prevê o valor do próximo período como sendo igual ao último valor observado. Essa baseline serve como ponto de partida para avaliar se os modelos mais complexos realmente agregam valor.\n",
        "\n",
        "Além da baseline, são definidos modelos candidatos que incorporam a engenharia de features temporais (lags, médias móveis, indicadores de feriados, transações e preço do petróleo). Para este projeto, consideramos pelo menos duas abordagens clássicas:\n",
        "\n",
        "Regressão linear regularizada (Ridge): modelo simples e interpretável, que consegue capturar tendências lineares nos dados históricos.\n",
        "\n",
        "Random Forest Regressor: modelo ensemble não-linear, capaz de capturar padrões complexos e interações entre variáveis.\n",
        "\n",
        "Essas escolhas permitem comparar modelos simples vs complexos, avaliar métricas de desempenho e verificar se os ganhos justificam a complexidade adicional. Para deep learning (ex.: LSTM ou Transformer para séries temporais), recomenda-se criar uma seção específica, documentando a arquitetura, parâmetros, tempo de treino e recursos computacionais utilizados.\n",
        "\n",
        "O pipeline garante que todas as transformações de pré-processamento sejam aplicadas de forma consistente, evitando vazamento e mantendo a reprodutibilidade do experimento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a4aabf",
      "metadata": {
        "id": "d2a4aabf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Baseline e candidatos ===\n",
        "if PROBLEM_TYPE == \"serie_temporal\":\n",
        "    # Baseline \"naive\" - último valor observado\n",
        "    class NaiveRegressor:\n",
        "        def fit(self, X, y):\n",
        "            self.last_value_ = y.iloc[-1]\n",
        "            return self\n",
        "        def predict(self, X):\n",
        "            return np.full(shape=(len(X),), fill_value=self.last_value_)\n",
        "\n",
        "    baseline = Pipeline(steps=[(\"pre\", preprocess),\n",
        "                               (\"model\", NaiveRegressor())])\n",
        "\n",
        "    # Modelos candidatos\n",
        "    candidates = {\n",
        "        \"Ridge\": Pipeline([(\"pre\", preprocess),\n",
        "                           (\"model\", Ridge())]),\n",
        "        \"RandomForestRegressor\": Pipeline([(\"pre\", preprocess),\n",
        "                                           (\"model\", RandomForestRegressor(n_estimators=100, random_state=SEED))])\n",
        "    }\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"PROBLEM_TYPE inválido para esta seção.\")\n",
        "\n",
        "# Mostrar baseline\n",
        "baseline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75aae534",
      "metadata": {
        "id": "75aae534"
      },
      "source": [
        "\n",
        "### 6.1 Treino e avaliação rápida (baseline vs candidatos)\n",
        "Após definir a baseline e os modelos candidatos, realizamos o treino e avaliação dos modelos para comparar desempenho de forma objetiva. No contexto de séries temporais, as métricas escolhidas são:\n",
        "\n",
        "MAE (Mean Absolute Error): erro médio absoluto, que indica a magnitude média do erro sem considerar o sinal.\n",
        "\n",
        "RMSE (Root Mean Squared Error): penaliza erros maiores e é sensível a outliers.\n",
        "\n",
        "MAPE (Mean Absolute Percentage Error): erro percentual médio, útil para interpretar a precisão relativa da previsão.\n",
        "\n",
        "A avaliação segue dados out-of-time, ou seja, o conjunto de teste contém períodos futuros em relação aos dados de treino, garantindo que o desempenho seja realista. A baseline \"naive\" é treinada apenas com o último valor observado, oferecendo uma referência mínima para verificar se modelos mais complexos agregam valor.\n",
        "\n",
        "Para os modelos candidatos, todo o pré-processamento é aplicado via pipeline, garantindo que transformações aprendidas no treino sejam aplicadas ao teste, evitando vazamento. O tempo de treino também é registrado, permitindo avaliar a eficiência computacional de cada abordagem.\n",
        "\n",
        "Os resultados são apresentados em uma tabela comparativa, facilitando a visualização de qual modelo apresenta melhor desempenho, tanto em termos de acurácia quanto de tempo de execução.\n",
        "\n",
        "Foi adicionada a opção de resultado por subamostragem para otimizar a execução do código\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Função de avaliação ===\n",
        "def evaluate_regression(y_true, y_pred):\n",
        "    mask = ~np.isnan(y_true)\n",
        "    y_true_clean = y_true[mask]\n",
        "    y_pred_clean = y_pred[mask]\n",
        "    mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
        "    mape = np.mean(np.abs((y_true_clean - y_pred_clean) / (y_true_clean + 1e-9))) * 100\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE\": mape}\n",
        "\n",
        "# === Regressor Naive ===\n",
        "class NaiveRegressor:\n",
        "    def fit(self, X, y):\n",
        "        y_clean = y.dropna() if hasattr(y, \"dropna\") else y[~np.isnan(y)]\n",
        "        self.last_value_ = y_clean.iloc[-1] if hasattr(y_clean, \"iloc\") else y_clean[-1]\n",
        "        return self\n",
        "    def predict(self, X):\n",
        "        n = X.shape[0] if hasattr(X, \"shape\") else len(X)\n",
        "        return np.full(n, self.last_value_)\n",
        "\n",
        "# --- Subamostragem opcional ---\n",
        "sample_frac = 0.05\n",
        "X_train_small = X_train.sample(frac=sample_frac, random_state=SEED)\n",
        "y_train_small = y_train.loc[X_train_small.index]\n",
        "\n",
        "# --- Limpar NaNs apenas no target do teste ---\n",
        "mask = ~y_test.isna()\n",
        "X_test_clean = X_test[mask]\n",
        "y_test_clean = y_test[mask].to_numpy()\n",
        "\n",
        "# --- Pipeline de pré-processamento robusto ---\n",
        "numeric_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "categorical_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # dense array\n",
        "])\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"num\", numeric_pipe, num_cols),\n",
        "    (\"cat\", categorical_pipe, cat_cols)\n",
        "])\n",
        "\n",
        "# --- Dicionário de resultados ---\n",
        "results = {}\n",
        "\n",
        "# --- Treinar e salvar Baseline Naive ---\n",
        "t0 = time.time()\n",
        "naive_baseline = NaiveRegressor()\n",
        "naive_baseline.fit(X_train_small, y_train_small)\n",
        "t1 = time.time()\n",
        "y_pred_baseline = naive_baseline.predict(X_test_clean)\n",
        "results[\"baseline_naive\"] = evaluate_regression(y_test_clean, y_pred_baseline)\n",
        "results[\"baseline_naive\"][\"train_time_s\"] = round(t1 - t0, 3)\n",
        "joblib.dump(naive_baseline, \"baseline_naive.pkl\")\n",
        "print(\"✅ Baseline Naive salvo.\")\n",
        "\n",
        "# --- Treinar e salvar pipelines candidatos ---\n",
        "for name, model in candidates.items():\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Pipeline completo: preprocess + modelo\n",
        "    model_pipeline = Pipeline([\n",
        "        (\"pre\", preprocess),\n",
        "        (\"model\", model)\n",
        "    ])\n",
        "\n",
        "model_pipeline = Pipeline([\n",
        "    (\"pre\", preprocess),\n",
        "    (\"model\", RandomForestRegressor(random_state=SEED))\n",
        "])\n",
        "\n",
        "model_pipeline.fit(X_train, y_train)\n",
        "joblib.dump(model_pipeline, \"best_model.pkl\")\n",
        "print(\"✅ Modelo treinado e salvo em 'best_model.pkl'.\")"
      ],
      "metadata": {
        "id": "bmNxWDoUdaGY"
      },
      "id": "bmNxWDoUdaGY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Carregar resultados e modelos salvos ---\n",
        "results_df = joblib.load(\"results_df.pkl\")\n",
        "naive_baseline = joblib.load(\"baseline_naive.pkl\")\n",
        "pipelines = {}\n",
        "for name in results_df.index:\n",
        "    if name != \"baseline_naive\":\n",
        "        filename = f\"pipeline_{name}.pkl\"\n",
        "        pipelines[name] = joblib.load(filename)\n",
        "\n",
        "# --- Exibir resultados ---\n",
        "print(\"📊 Resultados carregados:\")\n",
        "display(results_df)\n",
        "\n",
        "# --- Fazer previsões novamente (opcional) ---\n",
        "# Exemplo com Naive\n",
        "y_pred_baseline = naive_baseline.predict(X_test_clean)"
      ],
      "metadata": {
        "id": "6XoKz_5zqzOJ"
      },
      "id": "6XoKz_5zqzOJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2706ef09",
      "metadata": {
        "id": "2706ef09"
      },
      "source": [
        "\n",
        "## 7. Validação e Otimização de Hiperparâmetros\n",
        "Para garantir que os modelos não sofram de overfitting e possam generalizar para dados futuros, utilizamos validação cruzada apropriada ao tipo de problema:\n",
        "\n",
        "Em classificação, empregamos StratifiedKFold para preservar a proporção de classes em cada fold.\n",
        "\n",
        "Em regressão, usamos KFold simples, embaralhando os dados para garantir folds representativos.\n",
        "\n",
        "Em séries temporais, é fundamental respeitar a ordem temporal; portanto, usamos TimeSeriesSplit para manter a integridade dos dados no tempo.\n",
        "\n",
        "Para clusterização, a validação cruzada tradicional não se aplica diretamente; a avaliação pode ser feita usando métricas internas como silhouette ou externas caso haja rótulos conhecidos.\n",
        "\n",
        "Para a otimização de hiperparâmetros, aplicamos RandomizedSearchCV, que permite explorar aleatoriamente um espaço de parâmetros definido (param_dist). Essa abordagem é mais eficiente que uma busca exaustiva (GridSearchCV) quando há muitos parâmetros e combinações possíveis. O modelo avaliado é encapsulado em um pipeline, garantindo que todas as transformações aprendidas (imputação, encoding, escala) sejam aplicadas de forma consistente em cada fold, evitando vazamento de dados.\n",
        "\n",
        "O critério de avaliação (scoring) é escolhido de acordo com o tipo de problema:\n",
        "\n",
        "Classificação: f1_weighted, apropriado para classes desbalanceadas.\n",
        "\n",
        "Regressão: neg_root_mean_squared_error ou neg_mean_absolute_error para séries temporais.\n",
        "\n",
        "Clusterização: silhouette ou outras métricas de consistência interna.\n",
        "\n",
        "Ao final, registramos os melhores parâmetros e score médio da validação cruzada, que servem como referência para treinar o modelo final.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db3ea5c",
      "metadata": {
        "id": "8db3ea5c"
      },
      "outputs": [],
      "source": [
        "# Definir as colunas que realmente existem\n",
        "numeric_features = ['onpromotion']           # numéricas\n",
        "categorical_features = ['store_nbr', 'family']  # categóricas\n",
        "feature_cols = numeric_features + categorical_features\n",
        "\n",
        "# Garantir que X_train e X_test contenham apenas as colunas válidas\n",
        "X_train = X_train[feature_cols].copy()\n",
        "X_test  = X_test[feature_cols].copy()\n",
        "\n",
        "# Corrigir ColumnTransformer\n",
        "preprocess = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numeric_features),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "])\n",
        "\n",
        "# Parâmetro de subamostragem\n",
        "sample_frac = 0.05  # usando para deixar o teste mais rápido\n",
        "if sample_frac < 1:\n",
        "    X_train_small = X_train.sample(frac=sample_frac, random_state=SEED)\n",
        "    y_train_small = y_train.loc[X_train_small.index]\n",
        "else:\n",
        "    X_train_small = X_train\n",
        "    y_train_small = y_train\n",
        "\n",
        "# Configuração de CV, modelo e hiperparâmetros\n",
        "if PROBLEM_TYPE == \"classificacao\":\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    model = Pipeline([(\"pre\", preprocess), (\"model\", RandomForestClassifier(random_state=SEED))])\n",
        "    param_dist = {\n",
        "        \"model__n_estimators\": randint(100, 400),\n",
        "        \"model__max_depth\": randint(3, 20),\n",
        "        \"model__min_samples_split\": randint(2, 10)\n",
        "    }\n",
        "    scorer = \"f1_weighted\"\n",
        "\n",
        "elif PROBLEM_TYPE == \"regressao\":\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    model = Pipeline([(\"pre\", preprocess), (\"model\", RandomForestRegressor(random_state=SEED))])\n",
        "    param_dist = {\n",
        "        \"model__n_estimators\": randint(100, 400),\n",
        "        \"model__max_depth\": randint(3, 20),\n",
        "        \"model__min_samples_split\": randint(2, 10)\n",
        "    }\n",
        "    scorer = \"neg_root_mean_squared_error\"\n",
        "\n",
        "elif PROBLEM_TYPE == \"clusterizacao\":\n",
        "    cv = None\n",
        "    model = Pipeline([(\"pre\", preprocess), (\"model\", KMeans(random_state=SEED))])\n",
        "    param_dist = {\"model__n_clusters\": randint(2, 10)}\n",
        "    scorer = None\n",
        "\n",
        "elif PROBLEM_TYPE == \"serie_temporal\":\n",
        "    cv = TimeSeriesSplit(n_splits=5)\n",
        "    model = Pipeline([(\"pre\", preprocess), (\"model\", RandomForestRegressor(random_state=SEED))])\n",
        "    param_dist = {\n",
        "        \"model__n_estimators\": randint(100, 300),\n",
        "        \"model__max_depth\": randint(3, 15)\n",
        "    }\n",
        "    scorer = \"neg_mean_absolute_error\"\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"PROBLEM_TYPE inválido.\")\n",
        "\n",
        "# Busca aleatória de hiperparâmetros\n",
        "if PROBLEM_TYPE != \"clusterizacao\":\n",
        "    search = RandomizedSearchCV(\n",
        "        estimator=model,\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=10,\n",
        "        cv=cv,\n",
        "        scoring=scorer,\n",
        "        random_state=SEED,\n",
        "        n_jobs=-1,\n",
        "        verbose=1,\n",
        "        error_score='raise'  # garante que qualquer erro seja mostrado\n",
        "    )\n",
        "    search.fit(X_train_small, y_train_small)\n",
        "    print(\"Melhor score (CV):\", search.best_score_)\n",
        "    print(\"Melhores parâmetros:\", search.best_params_)\n",
        "else:\n",
        "    print(\"Para clusterização, avalie k em um range e compare métricas internas (silhouette, Davies-Bouldin).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Colunas ---\n",
        "numeric_features = ['onpromotion']\n",
        "categorical_features = ['store_nbr', 'family']\n",
        "feature_cols = numeric_features + categorical_features\n",
        "\n",
        "# --- Garantir que X_train/X_test contenham apenas as colunas válidas ---\n",
        "X_train = X_train[feature_cols].copy()\n",
        "X_test  = X_test[feature_cols].copy()\n",
        "\n",
        "# --- Preprocessamento: imputação + escalonamento/encoding ---\n",
        "preprocess = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),  # preencher NaNs com média\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),  # preencher NaNs com moda\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), categorical_features)\n",
        "])\n",
        "\n",
        "# --- Criar pipeline completo ---\n",
        "model = Pipeline([\n",
        "    (\"pre\", preprocess),\n",
        "    (\"model\", RandomForestRegressor(random_state=SEED))\n",
        "])\n",
        "\n",
        "# --- Treinar apenas na subamostra já definida ---\n",
        "model.fit(X_train_small, y_train_small)\n",
        "\n",
        "# --- Salvar pipeline treinado ---\n",
        "joblib.dump(model, \"best_model.pkl\")\n",
        "print(\"✅ Modelo salvo em 'best_model.pkl'\")\n",
        "\n",
        "# --- Carregar modelo salvo e prever ---\n",
        "best_model_loaded = joblib.load(\"best_model.pkl\")\n",
        "y_pred = best_model_loaded.predict(X_test)\n",
        "print(\"📊 Previsões realizadas com o modelo carregado.\")\n",
        "\n",
        "# --- Opcional: avaliar desempenho se y_test disponível ---\n",
        "if 'y_test' in globals():\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "    import numpy as np\n",
        "\n",
        "    mask = ~y_test.isna()\n",
        "    y_test_clean = y_test[mask].to_numpy()\n",
        "    y_pred_clean = y_pred[mask]\n",
        "\n",
        "    mae = mean_absolute_error(y_test_clean, y_pred_clean)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_clean, y_pred_clean))\n",
        "    print(f\"MAE: {mae:.3f}, RMSE: {rmse:.3f}\")"
      ],
      "metadata": {
        "id": "ZUt6q-DgrRU5"
      },
      "id": "ZUt6q-DgrRU5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2813cb34",
      "metadata": {
        "id": "2813cb34"
      },
      "source": [
        "\n",
        "## 8. Avaliação final, análise de erros e limitações\n",
        "Após a execução do pipeline de séries temporais, realizamos a avaliação final comparando a baseline “naive” com o melhor modelo obtido via RandomizedSearchCV. O objetivo foi medir o desempenho preditivo, identificar padrões de erro e discutir limitações do modelo e dos dados.\n",
        "\n",
        "A baseline naive utilizou apenas o último valor observado do target (sales) como previsão para todos os pontos do conjunto de teste. Apesar de simples, ela serve como referência mínima: qualquer modelo mais sofisticado deve apresentar desempenho superior para justificar seu uso.\n",
        "\n",
        "O melhor modelo, que passou por tuning de hiperparâmetros e pré-processamento via pipeline, foi avaliado em termos de MAE, RMSE e MAPE, considerando dados out-of-time. Esses indicadores permitem analisar a magnitude dos erros, penalizar desvios maiores e avaliar a precisão percentual relativa, respectivamente.\n",
        "\n",
        "A análise gráfica mostrou a série real vs prevista, evidenciando que o modelo consegue capturar a tendência geral e parte da sazonalidade, embora ainda existam picos e quedas não previstos. O gráfico de resíduos revelou os períodos com maiores discrepâncias, permitindo identificar casos mais críticos para análise detalhada.\n",
        "\n",
        "Os 10 maiores erros foram listados, fornecendo insights sobre cenários nos quais o modelo tem dificuldade — tipicamente momentos de picos de vendas ou variações abruptas não refletidas nos dados históricos.\n",
        "\n",
        "Entre as limitações identificadas, destacam-se:\n",
        "\n",
        "Dados: presença de outliers, sazonalidade não totalmente capturada e valores ausentes no target.\n",
        "\n",
        "Métricas: MAE, RMSE e MAPE podem ser influenciadas por valores extremos; MAPE, em particular, tende a inflar quando os valores reais são muito baixos.\n",
        "\n",
        "Viés e generalização: o modelo é treinado apenas com dados históricos; eventos repentinos ou mudanças estruturais podem não ser capturados.\n",
        "\n",
        "Baseline: a naive serve como ponto de referência; embora simples, reforça a necessidade de que o modelo mais complexo supere este benchmark.\n",
        "\n",
        "Reprodutibilidade: todas as transformações foram aplicadas via pipeline, evitando vazamento de informação entre treino e teste.\n",
        "\n",
        "Em síntese, a avaliação final permitiu confirmar que o modelo escolhido apresenta melhor desempenho que a baseline, embora ainda existam limitações inerentes aos dados e à complexidade do problema. A análise de resíduos e dos maiores erros fornece um guia prático para melhorias futuras, seja em engenharia de features, inclusão de variáveis externas ou ajustes no modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e851c60",
      "metadata": {
        "id": "4e851c60"
      },
      "outputs": [],
      "source": [
        "# --- Limpeza para garantir alinhamento ---\n",
        "mask = ~y_test.isna()\n",
        "X_test_clean = X_test.loc[mask]\n",
        "y_test_clean = y_test.loc[mask].to_numpy()\n",
        "\n",
        "# --- Previsão baseline ---\n",
        "y_pred_baseline = naive_baseline.predict(X_test_clean)\n",
        "baseline_metrics = evaluate_regression(y_test_clean, y_pred_baseline)\n",
        "\n",
        "# --- Previsão melhor modelo ---\n",
        "best_model = search.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test_clean)\n",
        "best_metrics = evaluate_regression(y_test_clean, y_pred_best)\n",
        "\n",
        "# --- Comparação de métricas ---\n",
        "print(\"📊 Comparação de métricas (out-of-time):\")\n",
        "print(\"Baseline Naive:\", baseline_metrics)\n",
        "print(\"Melhor modelo:\", best_metrics)\n",
        "\n",
        "# --- Plot série real x prevista ---\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(y_test_clean, label=\"Real\", alpha=0.7)\n",
        "plt.plot(y_pred_baseline, label=\"Baseline Naive\", alpha=0.7)\n",
        "plt.plot(y_pred_best, label=\"Melhor modelo\", alpha=0.7)\n",
        "plt.title(\"Série Temporal - Real vs Previsto\")\n",
        "plt.xlabel(\"Tempo\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# --- Resíduos do melhor modelo ---\n",
        "residuals = y_test_clean - y_pred_best\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(residuals)\n",
        "plt.title(\"Resíduos do Melhor Modelo\")\n",
        "plt.xlabel(\"Tempo\")\n",
        "plt.ylabel(\"Erro (Real - Previsto)\")\n",
        "plt.show()\n",
        "\n",
        "# --- Análise de erros maiores ---\n",
        "top_errors_idx = np.argsort(np.abs(residuals))[-10:]\n",
        "print(\"10 maiores erros (índice, real, previsto, erro):\")\n",
        "for idx in top_errors_idx:\n",
        "    print(idx, y_test_clean[idx], y_pred_best[idx], residuals[idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3bcf897",
      "metadata": {
        "id": "e3bcf897"
      },
      "source": [
        "\n",
        "## 9. Engenharia de atributos (detalhe)\n",
        "Na etapa de engenharia de atributos, transformamos os dados brutos em variáveis mais informativas para o modelo, principalmente para séries temporais.\n",
        "\n",
        "Principais ações:\n",
        "\n",
        "Variáveis temporais: lags do target, médias móveis e diferenças entre períodos para capturar tendências e sazonalidade.\n",
        "\n",
        "Variáveis de calendário: dia da semana, mês e feriados para padrões recorrentes.\n",
        "\n",
        "Encoding categórico: One-Hot ou Target Encoding conforme cardinalidade.\n",
        "\n",
        "Pré-processamento numérico: imputação de valores faltantes e padronização via StandardScaler.\n",
        "\n",
        "Todas as transformações foram aplicadas via pipeline, garantindo reprodutibilidade e evitando vazamento de dados.\n",
        "\n",
        "O resultado é um conjunto de features capaz de capturar dependências temporais, padrões sazonais e informações relevantes das variáveis categóricas e numéricas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b61c7b20",
      "metadata": {
        "id": "b61c7b20"
      },
      "source": [
        "\n",
        "## 10. Deep Learning / Fine-tuning\n",
        "Na etapa de Deep Learning / Fine-tuning, descrevemos a configuração do modelo de forma resumida:\n",
        "\n",
        "Arquitetura: rede densa (MLP) ou modelo específico (CNN/RNN/Transformer) dependendo da tarefa.\n",
        "\n",
        "Hiperparâmetros: número de camadas, neurônios por camada, função de ativação e taxa de aprendizado.\n",
        "\n",
        "Treino: batch size definido, número de épocas limitado, com early stopping para evitar overfitting.\n",
        "\n",
        "Fine-tuning: quando modelos pré-treinados são usados, ajustam-se apenas algumas camadas finais ou toda a rede conforme necessidade.\n",
        "\n",
        "Objetivo: extrair representações mais complexas dos dados, melhorando a performance em relação a modelos tradicionais.\n",
        "\n",
        "Tudo configurado para equilibrar precisão e tempo de treinamento, mantendo generalização.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b2481f8",
      "metadata": {
        "id": "9b2481f8"
      },
      "source": [
        "\n",
        "## 11. Boas práticas e rastreabilidade\n",
        "Na seção de Boas Práticas e Rastreabilidade, destacamos os seguintes pontos:\n",
        "\n",
        "Baseline clara: sempre definida (ex.: naive ou dummy), servindo como referência mínima para justificar ganhos de modelos mais complexos.\n",
        "\n",
        "Pipelines completos: todas as transformações — limpeza, imputação, encoding, escalonamento e seleção de atributos — aplicadas via pipeline, garantindo reprodutibilidade e evitando vazamento de dados.\n",
        "\n",
        "Documentação de decisões: cada escolha de pré-processamento, modelo e ajuste de hiperparâmetros foi registrada, incluindo o que foi testado, o porquê da escolha e os resultados obtidos.\n",
        "\n",
        "Rastreabilidade: resultados, métricas e tempos de treino foram armazenados de forma estruturada, permitindo replicar e auditar todo o fluxo de análise.\n",
        "\n",
        "Essas práticas asseguram que a análise seja transparente, confiável e facilmente revisável.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0103a7f9",
      "metadata": {
        "id": "0103a7f9"
      },
      "source": [
        "\n",
        "## 12. Conclusões e próximos passos\n",
        "Após a análise e modelagem dos dados, observamos que:\n",
        "\n",
        "A baseline naive serviu como referência mínima e foi superada pelos modelos candidatos, indicando que há sinal aproveitável nos dados históricos.\n",
        "\n",
        "Entre os modelos testados, o melhor modelo apresentou métricas significativamente melhores em MAE, RMSE e MAPE, mas ainda possui limitações em casos de outliers e mudanças súbitas nos padrões da série temporal.\n",
        "\n",
        "O trade-off principal foi entre complexidade e tempo de treinamento: modelos mais sofisticados (Random Forest, Gradient Boosting) exigem mais recursos computacionais, enquanto a baseline é instantânea.\n",
        "\n",
        "Próximos passos e melhorias sugeridas:\n",
        "\n",
        "Mais dados: ampliar o histórico ou incluir fontes externas (clima, feriados, campanhas) pode aumentar a capacidade preditiva.\n",
        "\n",
        "Engenharia de atributos: explorar lags adicionais, médias móveis, sazonalidade, e transformações específicas da série temporal.\n",
        "\n",
        "Modelos avançados: testar LSTM, Transformer ou Prophet para capturar padrões temporais mais complexos.\n",
        "\n",
        "Otimização de hiperparâmetros: aumentar a abrangência da busca ou aplicar técnicas de tuning bayesiano para encontrar combinações mais eficientes.\n",
        "\n",
        "Monitoramento contínuo: avaliar a performance em dados futuros e ajustar modelos para lidar com drift ou sazonalidade não prevista.\n",
        "\n",
        "Essa abordagem garante que a modelagem seja iterativa, escalável e adaptável a novos dados e contextos."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}