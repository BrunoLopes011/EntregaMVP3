{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6dab859",
      "metadata": {
        "id": "f6dab859"
      },
      "source": [
        "\n",
        "# Template ‚Äî MVP: *Machine Learning & Analytics*\n",
        "**Autor:** Bruno dos Santos Lopes\n",
        "\n",
        "**Data:** 27/09/2025\n",
        "\n",
        "**Matr√≠cula:** 4052025000271\n",
        "\n",
        "**Dataset:** [Store Sales - Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data)\n",
        "\n",
        "> **Importante:**\n",
        "A estrutura base deste notebook pode servir como um guia inicial para desenvolver suas an√°lises, j√° que contempla grande parte das sugest√µes do checklist apresentado no enunciado do MVP. Entretanto, √© importante destacar que esta estrutura √© apenas um ponto de partida: poder√£o ser necess√°rias etapas e an√°lises adicionais al√©m das aqui exemplificadas.\n",
        "\n",
        "> O essencial √© garantir profundidade nas discuss√µes e an√°lises, construindo um storytelling consistente que explore os principais conceitos e t√©cnicas vistos nas aulas da Sprint de Machine Learning & Analytics.\n",
        "\n",
        "> Lembre-se: n√£o existe uma receita pronta. A ordem e as se√ß√µes detalhadas abaixo s√£o apenas sugest√µes. O problema escolhido e a hist√≥ria que voc√™ deseja contar devem guiar, em grande parte, a forma final do seu trabalho.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9e69fe5",
      "metadata": {
        "id": "e9e69fe5"
      },
      "source": [
        "\n",
        "## ‚úÖ Checklist do MVP (o que precisa conter)\n",
        "- [ ] **Problema definido** e contexto de neg√≥cio\n",
        "- [ ] **Carga e prepara√ß√£o** dos dados (sem vazamento de dados)\n",
        "- [ ] **Divis√£o** em treino/valida√ß√£o/teste (ou valida√ß√£o cruzada apropriada)\n",
        "- [ ] **Tratamento**: limpeza, transforma√ß√£o e **engenharia de atributos**\n",
        "- [ ] **Modelagem**: comparar abordagens/modelos (com **baseline**)\n",
        "- [ ] **Otimiza√ß√£o de hiperpar√¢metros**\n",
        "- [ ] **Avalia√ß√£o** com **m√©tricas adequadas** e discuss√£o de limita√ß√µes\n",
        "- [ ] **Boas pr√°ticas**: seeds fixas, tempo de treino, recursos computacionais, documenta√ß√£o\n",
        "- [ ] **Pipelines reprodut√≠veis** (sempre que poss√≠vel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a670b1e8",
      "metadata": {
        "id": "a670b1e8"
      },
      "source": [
        "\n",
        "## 1. Escopo, objetivo e defini√ß√£o do problema\n",
        "**TODO:** Explique brevemente:\n",
        "- Contexto do problema e objetivo:\n",
        "O problema central abordado neste MVP √© a ruptura de estoque, situa√ß√£o em que a demanda de um produto n√£o pode ser atendida devido √† falta de reposi√ß√£o adequada. Isso gera perda de vendas, redu√ß√£o da satisfa√ß√£o do cliente e inefici√™ncias na cadeia log√≠stica.\n",
        "O objetivo do MVP √© desenvolver uma solu√ß√£o baseada em dados que permita prever a demanda e apoiar a reposi√ß√£o de produtos, reduzindo a probabilidade de ruptura.\n",
        "Empresas de varejo precisam prever a demanda de produtos em diferentes lojas para otimizar estoque, reduzir perdas e maximizar o faturamento. O dataset Store Sales ‚Äì Time Series Forecasting, disponibilizado pelo Kaggle, cont√©m dados hist√≥ricos de vendas di√°rias por loja e fam√≠lia de produtos, incluindo informa√ß√µes de promo√ß√µes, feriados e indicadores externos.\n",
        "O objetivo deste projeto √© desenvolver modelos de previs√£o de vendas (forecasting) que permitam estimar a demanda futura por produto/loja, considerando tend√™ncias, sazonalidades e eventos externos.  \n",
        "\n",
        "- Tipo de tarefa: Problema de previs√£o de s√©ries temporais (forecasting)\n",
        "- Natureza do problema: Regress√£o (vari√°vel-alvo = valor num√©rico de vendas).  \n",
        "- √Årea de aplica√ß√£o: Dados tabulares e temporais e Aplica√ß√£o em Analytics e Intelig√™ncia de Neg√≥cios (BI/AI para varejo).  \n",
        "- Gest√£o de estoque eficiente: evitar ruptura (falta de produtos) e excesso (desperd√≠cio e custos). Planejamento de promo√ß√µes: prever impacto de descontos e campanhas sazonais. Aloca√ß√£o de recursos: otimizar log√≠stica, distribui√ß√£o e compras junto a fornecedores. Suporte √† tomada de decis√£o: fornecer previs√µes confi√°veis para gestores de diferentes √°reas (vendas, marketing, supply chain).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1bd6e05",
      "metadata": {
        "id": "e1bd6e05"
      },
      "source": [
        "\n",
        "## 2. Reprodutibilidade e ambiente\n",
        "Especifique o ambiente. Por exemplo:\n",
        "- Bibliotecas usadas.\n",
        "- Seeds fixas para reprodutibilidade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b5cec9",
      "metadata": {
        "id": "42b5cec9"
      },
      "outputs": [],
      "source": [
        "# === Setup b√°sico e reprodutibilidade ===\n",
        "import os, random, sys, math, time, warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "# Scikit-learn (pr√©-processamento, modelagem, avalia√ß√£o)\n",
        "from sklearn.model_selection import (train_test_split, StratifiedKFold, KFold, TimeSeriesSplit, RandomizedSearchCV)\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score, confusion_matrix,\n",
        "    mean_absolute_error, mean_squared_error, r2_score,\n",
        "    silhouette_score\n",
        ")\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "# Modelagem de s√©ries temporais\n",
        "import statsmodels.api as sm\n",
        "from prophet import Prophet\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Configura√ß√µes globais\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "# Para frameworks adicionais:\n",
        "# import torch; torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "# import tensorflow as tf; tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"Seed global:\", SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Fun√ß√µes python (opcional)\n",
        "Defina, se necess√°rio, fun√ß√µes em Python para reutilizar seu c√≥digo e torn√°-lo mais organizado. Essa √© uma boa pr√°tica de programa√ß√£o que facilita a leitura, manuten√ß√£o e evolu√ß√£o do seu projeto."
      ],
      "metadata": {
        "id": "yTTTI_gjtEbp"
      },
      "id": "yTTTI_gjtEbp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fun√ß√£o para criar lags de s√©ries temporais\n",
        "def create_lag_features(df, group_cols, target_col, lags=[1,7,14,28]):\n",
        "    \"\"\"\n",
        "    Cria colunas de lags para s√©ries temporais agrupadas.\n",
        "\n",
        "    Par√¢metros:\n",
        "    - df: DataFrame contendo a s√©rie temporal\n",
        "    - group_cols: colunas para agrupar (ex.: ['store_nbr', 'family'])\n",
        "    - target_col: coluna alvo (ex.: 'sales')\n",
        "    - lags: lista de per√≠odos de lag\n",
        "\n",
        "    Retorno:\n",
        "    - DataFrame com novas colunas de lags\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    for lag in lags:\n",
        "        df_copy[f'{target_col}_lag_{lag}'] = df_copy.groupby(group_cols)[target_col].shift(lag)\n",
        "    return df_copy\n",
        "\n",
        "# Fun√ß√£o para calcular m√©tricas de previs√£o\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "def evaluate_forecast(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calcula RMSE, MAE e MAPE de uma previs√£o.\n",
        "\n",
        "    Par√¢metros:\n",
        "    - y_true: valores reais\n",
        "    - y_pred: valores previstos\n",
        "\n",
        "    Retorno:\n",
        "    - dicion√°rio com m√©tricas\n",
        "    \"\"\"\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = (abs(y_true - y_pred) / (y_true + 1e-9)).mean() * 100  # evita divis√£o por zero\n",
        "    return {'RMSE': rmse, 'MAE': mae, 'MAPE': mape}\n",
        "\n",
        "    # Fun√ß√£o para plotar s√©rie temporal\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_series(df, date_col, target_col, title=None, figsize=(12,5)):\n",
        "    \"\"\"\n",
        "    Plota s√©rie temporal.\n",
        "\n",
        "    Par√¢metros:\n",
        "    - df: DataFrame\n",
        "    - date_col: coluna de datas\n",
        "    - target_col: coluna de valores\n",
        "    - title: t√≠tulo opcional\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.plot(df[date_col], df[target_col], marker='o', linestyle='-')\n",
        "    plt.xlabel('Data')\n",
        "    plt.ylabel(target_col)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6PQYS9MftOLB"
      },
      "id": "6PQYS9MftOLB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ad1d8aa8",
      "metadata": {
        "id": "ad1d8aa8"
      },
      "source": [
        "\n",
        "## 3. Dados: carga, entendimento e qualidade\n",
        "O presente projeto utiliza o dataset Store Sales ‚Äî Time Series Forecasting, disponibilizado no Kaggle, cujo objetivo √© prever a demanda di√°ria de produtos em diferentes lojas, permitindo otimizar estoques, reduzir perdas e apoiar decis√µes estrat√©gicas de marketing e log√≠stica. Os dados foram carregados diretamente do GitHub, garantindo a execu√ß√£o imediata do notebook no Google Colab, sem necessidade de configura√ß√£o adicional.\n",
        "\n",
        "O dataset √© composto por m√∫ltiplos arquivos CSV, cada um com informa√ß√µes complementares sobre vendas, lojas, feriados, eventos e pre√ßos de petr√≥leo. O arquivo train.csv cont√©m as vendas hist√≥ricas di√°rias por loja e fam√≠lia de produtos, sendo a vari√°vel-alvo a coluna sales. O arquivo test.csv corresponde ao conjunto de teste, utilizado para submiss√£o de previs√µes. O arquivo stores.csv fornece dados adicionais sobre as lojas, incluindo cidade, estado, tipo e cluster de classifica√ß√£o. O arquivo holidays_events.csv lista feriados nacionais e regionais, bem como eventos especiais, indicando se foram transferidos. O arquivo transactions.csv informa o n√∫mero di√°rio de transa√ß√µes por loja, e o arquivo oil.csv registra o pre√ßo di√°rio do petr√≥leo WTI, que pode impactar o comportamento de vendas de certos produtos. Por fim, o arquivo sample_submission.csv serve como template para submiss√£o das previs√µes no Kaggle.\n",
        "\n",
        "As principais vari√°veis presentes no dataset incluem: store_nbr (n√∫mero da loja), family (categoria do produto), date (data da venda), sales (quantidade vendida), city, state, type e cluster das lojas, al√©m de vari√°veis relacionadas a feriados (type, locale, locale_name, transferred), n√∫mero de transa√ß√µes di√°rias e pre√ßo do petr√≥leo (dcoilwtico).\n",
        "\n",
        "Uma an√°lise preliminar da qualidade dos dados indicou a presen√ßa de valores ausentes em algumas colunas, como o pre√ßo do petr√≥leo, que ser√° tratado via interpola√ß√£o temporal. Zeros nas vendas podem indicar dias sem vendas ou feriados, e datas de feriados transferidos foram cuidadosamente consideradas para evitar inconsist√™ncias. Todas as transforma√ß√µes e cria√ß√£o de features de lag ser√£o realizadas apenas com informa√ß√µes hist√≥ricas, evitando vazamento de dados e garantindo a reprodutibilidade das previs√µes.\n",
        "\n",
        "Do ponto de vista √©tico, os dados s√£o p√∫blicos, voltados para fins educacionais e n√£o cont√™m informa√ß√µes pessoais ou sens√≠veis. O uso das informa√ß√µes √© restrito √† an√°lise de s√©ries temporais de vendas e √† constru√ß√£o de modelos de previs√£o de demanda, respeitando a confidencialidade e a integridade dos dados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e03c90f7",
      "metadata": {
        "id": "e03c90f7"
      },
      "outputs": [],
      "source": [
        "#Carga dos dados\n",
        "# URL base dos arquivos CSV no GitHub\n",
        "base_url = \"https://raw.githubusercontent.com/BrunoLopes011/EntregaMVP3/main/\"\n",
        "\n",
        "# Lista de arquivos\n",
        "arquivos = [\n",
        "    \"holidays_events.csv\",\n",
        "    \"oil.csv\",\n",
        "    \"sample_submission.csv\",\n",
        "    \"stores.csv\",\n",
        "    \"test.csv\",\n",
        "    \"train.csv\",\n",
        "    \"transactions.csv\"\n",
        "]\n",
        "\n",
        "# Leitura dos arquivos com pandas\n",
        "print(\"üì• Lendo arquivos diretamente do GitHub...\")\n",
        "holidays_events = pd.read_csv(base_url + \"holidays_events.csv\")\n",
        "oil = pd.read_csv(base_url + \"oil.csv\")\n",
        "sample_submission = pd.read_csv(base_url + \"sample_submission.csv\")\n",
        "stores = pd.read_csv(base_url + \"stores.csv\")\n",
        "test = pd.read_csv(base_url + \"test.csv\")\n",
        "train = pd.read_csv(base_url + \"train.csv\")\n",
        "transactions = pd.read_csv(base_url + \"transactions.csv\")\n",
        "\n",
        "print(\"‚úÖ Arquivos carregados com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prints dos cabe√ßalhos dos arquivos\n",
        "\n",
        "print(\"üè¨ Vendas hist√≥ricas (train)\")\n",
        "display(train.head())\n",
        "\n",
        "print(\"üß™ Conjunto de teste (test)\")\n",
        "display(test.head())\n",
        "\n",
        "print(\"üè™ Informa√ß√µes das lojas (stores)\")\n",
        "display(stores.head())\n",
        "\n",
        "print(\"üìÖ Feriados e eventos (holidays_events)\")\n",
        "display(holidays_events.head())\n",
        "\n",
        "print(\"üí∞ Pre√ßo do petr√≥leo (oil)\")\n",
        "display(oil.head())\n",
        "\n",
        "print(\"üí≥ N√∫mero de transa√ß√µes por loja (transactions)\")\n",
        "display(transactions.head())\n",
        "\n",
        "print(\"üìÑ Template de submiss√£o (sample_submission)\")\n",
        "display(sample_submission.head())\n",
        "\n",
        "\n",
        "# Total de Inst√¢ncias e Atributos por Arquivo\n",
        "\n",
        "# Dicion√°rio com os datasets carregados\n",
        "datasets = {\n",
        "    \"train.csv\": train,\n",
        "    \"test.csv\": test,\n",
        "    \"stores.csv\": stores,\n",
        "    \"holidays_events.csv\": holidays_events,\n",
        "    \"transactions.csv\": transactions,\n",
        "    \"oil.csv\": oil,\n",
        "    \"sample_submission.csv\": sample_submission\n",
        "}\n",
        "\n",
        "# Fun√ß√£o para exibir tamanho dos datasets\n",
        "print(\"üìä Total de Inst√¢ncias e Atributos por Arquivo:\\n\")\n",
        "\n",
        "for nome, df in datasets.items():\n",
        "    print(f\"üìÅ {nome}:\")\n",
        "    print(f\"   üî¢ {df.shape[0]:,} registros\")\n",
        "    print(f\"   üìê {df.shape[1]} colunas\\n\")\n"
      ],
      "metadata": {
        "id": "yeaiTenpV3Vu"
      },
      "id": "yeaiTenpV3Vu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divis√£o dos dados\n",
        "\n",
        "A divis√£o dos dados foi realizada em treino (70%) e teste (30%).\n",
        "Embora a valida√ß√£o cruzada (k-fold) seja uma t√©cnica recomendada para reduzir a vari√¢ncia das estimativas, neste projeto optamos por n√£o utiliz√°-la devido ao grande volume de dados do Instacart, que tornaria a execu√ß√£o invi√°vel computacionalmente dentro do escopo do MVP. Ainda assim, mantivemos uma amostra de valida√ß√£o para evitar data leakage e garantir avalia√ß√£o robusta."
      ],
      "metadata": {
        "id": "zN5ylVoYcwxW"
      },
      "id": "zN5ylVoYcwxW"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Selecionar colunas que realmente existem\n",
        "wanted_cols = ['onpromotion', 'store_nbr', 'family']\n",
        "feature_cols = [c for c in wanted_cols if c in df.columns]\n",
        "\n",
        "# Definir X e y\n",
        "X = df[feature_cols].copy()\n",
        "y = df['sales'].copy()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Separar features num√©ricas e categ√≥ricas existentes\n",
        "numeric_features = [c for c in ['onpromotion'] if c in X_train.columns]\n",
        "categorical_features = [c for c in ['store_nbr', 'family'] if c in X_train.columns]\n",
        "\n",
        "# Filtrar X_train e X_test para apenas essas colunas (mantendo DataFrame)\n",
        "X_train = X_train[numeric_features + categorical_features].copy()\n",
        "X_test  = X_test[numeric_features + categorical_features].copy()\n",
        "\n",
        "numeric_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"num\", numeric_pipe, numeric_features),\n",
        "    (\"cat\", categorical_pipe, categorical_features)\n",
        "])\n",
        "\n",
        "print(\"‚úÖ ColumnTransformer pronto.\")\n",
        "print(\"Colunas num√©ricas:\", numeric_features)\n",
        "print(\"Colunas categ√≥ricas:\", categorical_features)\n",
        "\n"
      ],
      "metadata": {
        "id": "nF21lhtoc46t"
      },
      "id": "nF21lhtoc46t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a0bb41",
      "metadata": {
        "id": "c2a0bb41"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Verifica√ß√µes iniciais ===\n",
        "def explorar_dataset(df, nome_arquivo):\n",
        "    print(f\"üìÇ Explorando: {nome_arquivo}\\n\")\n",
        "\n",
        "    print(\"üîπ Amostra aleat√≥ria de 5 registros:\")\n",
        "    display(df.sample(5))\n",
        "\n",
        "    print(\"\\nüîπ Formato do dataset:\")\n",
        "    print(df.shape)\n",
        "\n",
        "    print(\"\\nüîπ Tipos de dados das colunas:\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    print(\"\\nüîπ Valores ausentes por coluna:\")\n",
        "    print(df.isna().sum())\n",
        "\n",
        "# Exemplo de uso:\n",
        "explorar_dataset(train, \"train.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tratamento de dados\n",
        "\n",
        "Foi aplicada padroniza√ß√£o nos atributos num√©ricos, de modo a evitar que vari√°veis em escalas diferentes prejudiquem o desempenho dos modelos (principalmente Regress√£o Log√≠stica). Al√©m disso, mantivemos vers√µes transformadas do dataset para posterior avalia√ß√£o."
      ],
      "metadata": {
        "id": "b5_WfAuGdLWs"
      },
      "id": "b5_WfAuGdLWs"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Identificar colunas num√©ricas e categ√≥ricas\n",
        "num_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "cat_features = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Criar transformador\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Aplicar transforma√ß√£o\n",
        "X_train_scaled = preprocessor.fit_transform(X_train)\n",
        "X_test_scaled = preprocessor.transform(X_test)"
      ],
      "metadata": {
        "id": "DZ6fQ0YidOJ7"
      },
      "id": "DZ6fQ0YidOJ7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelagem\n",
        "\n",
        "Para garantir reprodutibilidade e evitar retrabalho em diferentes etapas, utilizamos Pipeline do sklearn, que une pr√©-processamento e modelagem em um fluxo √∫nico. Isso facilita tanto o treino quanto a otimiza√ß√£o de hiperpar√¢metros."
      ],
      "metadata": {
        "id": "dHIjXESDdhkP"
      },
      "id": "dHIjXESDdhkP"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pipeline Random Forest\n",
        "rf_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler(with_mean=False)),  # opcional para RF\n",
        "    ('rf', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Pipeline Regress√£o Log√≠stica\n",
        "lr_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n"
      ],
      "metadata": {
        "id": "cr-eaedCddrY"
      },
      "id": "cr-eaedCddrY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4I80l0EhddXv"
      },
      "id": "4I80l0EhddXv"
    },
    {
      "cell_type": "markdown",
      "id": "4922028c",
      "metadata": {
        "id": "4922028c"
      },
      "source": [
        "\n",
        "### 3.1 An√°lise explorat√≥ria resumida (EDA)\n",
        "A an√°lise explorat√≥ria (EDA) tem como objetivo entender a distribui√ß√£o das vendas, identificar padr√µes sazonais e correlacionar fatores externos que podem influenciar a demanda. Nesta se√ß√£o, focamos em insights que impactam diretamente a modelagem de s√©ries temporais e a cria√ß√£o de features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b93654e5",
      "metadata": {
        "id": "b93654e5"
      },
      "outputs": [],
      "source": [
        "# Total de vendas agregadas por dia\n",
        "vendas_diarias = train.groupby(\"date\")[\"sales\"].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(vendas_diarias[\"date\"], vendas_diarias[\"sales\"], marker='', linestyle='-')\n",
        "plt.title(\"Vendas Totais por Dia\")\n",
        "plt.xlabel(\"Data\")\n",
        "plt.ylabel(\"Vendas\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Vendas hist√≥ricas\n",
        "# Boxplot por fam√≠lia de produtos (train)\n",
        "plt.figure(figsize=(16,6))\n",
        "train.boxplot(column=\"sales\", by=\"family\", rot=90)\n",
        "plt.title(\"Distribui√ß√£o de Vendas por Fam√≠lia de Produtos\")\n",
        "plt.suptitle(\"\")\n",
        "plt.ylabel(\"Vendas\")\n",
        "plt.show()\n",
        "\n",
        "#Distribui√ß√£o das vendas por fam√≠lia de produtos\n",
        "# Contagem de registros por loja\n",
        "train[\"store_nbr\"].value_counts().sort_index().plot(kind=\"bar\", figsize=(12,4))\n",
        "plt.title(\"N√∫mero de Registros por Loja\")\n",
        "plt.xlabel(\"Loja\")\n",
        "plt.ylabel(\"Quantidade de Dias\")\n",
        "plt.show()\n",
        "\n",
        "# Impacto de feriados e eventos\n",
        "# Vendas m√©dias em feriados vs dias normais\n",
        "feriados = holidays_events[holidays_events[\"type\"].isin([\"Holiday\", \"Event\"])]\n",
        "dias_feriado = train[train[\"date\"].isin(feriados[\"date\"])]\n",
        "dias_normais = train[~train[\"date\"].isin(feriados[\"date\"])]\n",
        "\n",
        "print(\"M√©dia de vendas em dias de feriado/evento:\", dias_feriado[\"sales\"].mean())\n",
        "print(\"M√©dia de vendas em dias normais:\", dias_normais[\"sales\"].mean())\n",
        "\n",
        "#Transa√ß√µes x Vendas\n",
        "# Agrupar por dia e loja\n",
        "df_trans_vendas = train.merge(transactions, on=[\"date\",\"store_nbr\"], how=\"left\")\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.scatter(df_trans_vendas[\"transactions\"], df_trans_vendas[\"sales\"], alpha=0.3)\n",
        "plt.title(\"Rela√ß√£o: Transa√ß√µes x Vendas\")\n",
        "plt.xlabel(\"Transa√ß√µes\")\n",
        "plt.ylabel(\"Vendas\")\n",
        "plt.show()\n",
        "\n",
        "#Pre√ßo do petr√≥leo x Vendas\n",
        "# Merge di√°rio com m√©dia de vendas\n",
        "df_oil = train.groupby(\"date\")[\"sales\"].sum().reset_index().merge(oil, on=\"date\", how=\"left\")\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(df_oil[\"date\"], df_oil[\"sales\"], label=\"Vendas Totais\")\n",
        "plt.plot(df_oil[\"date\"], df_oil[\"dcoilwtico\"]*1000, label=\"Pre√ßo do Petr√≥leo (escala ajustada)\", color=\"orange\")\n",
        "plt.title(\"Vendas Totais vs Pre√ßo do Petr√≥leo\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92d88464",
      "metadata": {
        "id": "92d88464"
      },
      "source": [
        "\n",
        "## 4. Defini√ß√£o do target, vari√°veis e divis√£o dos dados\n",
        "Para este projeto, a tarefa escolhida √© previs√£o de s√©ries temporais (forecasting), com o objetivo de estimar a quantidade de vendas di√°rias (sales) por loja e fam√≠lia de produtos. A vari√°vel-alvo definida √©, portanto, sales, enquanto as features incluem quatro colunas relevantes derivadas dos dados originais, excluindo a coluna de data (date) para evitar vazamento de informa√ß√µes futuras.\n",
        "\n",
        "A divis√£o dos dados respeitou a ordem temporal, de modo a n√£o embaralhar os registros, garantindo que o modelo utilize apenas informa√ß√µes passadas para prever o futuro. Foi realizado um corte temporal simples, com 80% dos registros para treino e 20% para teste, resultando em 438.208 observa√ß√µes de treino e 109.553 observa√ß√µes de teste. Essa abordagem preserva a sequ√™ncia cronol√≥gica essencial em s√©ries temporais.\n",
        "\n",
        "Al√©m disso, foi aplicada uma valida√ß√£o cruzada temporal utilizando TimeSeriesSplit com 5 folds, permitindo avaliar a robustez do modelo em diferentes per√≠odos hist√≥ricos. Cada fold manteve a ordem temporal e foi estruturado da seguinte forma:\n",
        "\n",
        "- Fold 1: treino=7938 | valida√ß√£o=7935\n",
        "- Fold 2: treino=15873 | valida√ß√£o=7935\n",
        "- Fold 3: treino=23808 | valida√ß√£o=7935\n",
        "- Fold 4: treino=31743 | valida√ß√£o=7935\n",
        "- Fold 5: treino=39678 | valida√ß√£o=7935\n",
        "\n",
        "Todas as transforma√ß√µes aplicadas aos dados, como cria√ß√£o de features e normaliza√ß√µes, foram ajustadas apenas nos dados de treino e posteriormente aplicadas aos conjuntos de valida√ß√£o e teste, garantindo reprodutibilidade e aus√™ncia de vazamento de informa√ß√µes. Para maior organiza√ß√£o e consist√™ncia, recomenda-se o uso de pipelines, integrando pr√©-processamento, engenharia de features e modelagem em um fluxo √∫nico.\n",
        "\n",
        "Essa configura√ß√£o permite que o modelo aprenda padr√µes hist√≥ricos, capture sazonalidade e tend√™ncias de vendas, e seja avaliado de forma realista em dados futuros, fornecendo previs√µes confi√°veis para suporte √† decis√£o em opera√ß√µes comerciais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5256d179",
      "metadata": {
        "id": "5256d179"
      },
      "outputs": [],
      "source": [
        "# === Defini√ß√£o do tipo de problema ===\n",
        "PROBLEM_TYPE = \"serie_temporal\"  # previs√£o de vendas\n",
        "\n",
        "# Target e features\n",
        "target = \"sales\"\n",
        "features = [c for c in train.columns if c not in [target, \"date\"]]\n",
        "\n",
        "print(\"PROBLEM_TYPE:\", PROBLEM_TYPE)\n",
        "print(\"Target:\", target)\n",
        "print(\"N features:\", len(features))\n",
        "\n",
        "# Ordenar dados por data\n",
        "train_sorted = train.sort_values(\"date\")\n",
        "\n",
        "# Corte temporal: 80% treino, 20% teste\n",
        "cutoff = int(len(train_sorted) * 0.8)\n",
        "train_ts = train_sorted.iloc[:cutoff]\n",
        "test_ts = train_sorted.iloc[cutoff:]\n",
        "\n",
        "X_train, y_train = train_ts[features], train_ts[target]\n",
        "X_test, y_test   = test_ts[features], test_ts[target]\n",
        "\n",
        "print(\"Treino:\", X_train.shape, \"| Teste:\", X_test.shape)\n",
        "\n",
        "# Exemplo de valida√ß√£o cruzada temporal\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n",
        "    print(f\"Fold {fold}: treino={len(train_idx)} | valida√ß√£o={len(val_idx)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a6a61e2",
      "metadata": {
        "id": "2a6a61e2"
      },
      "source": [
        "\n",
        "## 5. Tratamento de dados e **Pipeline** de pr√©-processamento\n",
        "Para garantir reprodutibilidade e evitar vazamento de dados, todos os passos de pr√©-processamento foram organizados em pipelines utilizando scikit-learn. O pipeline inclui as seguintes etapas:\n",
        "\n",
        "Limpeza e imputa√ß√£o: valores ausentes nas colunas num√©ricas s√£o preenchidos com a mediana, enquanto valores ausentes em colunas categ√≥ricas s√£o preenchidos com a moda (valor mais frequente).\n",
        "\n",
        "Escalonamento: colunas num√©ricas s√£o padronizadas usando StandardScaler para melhorar a converg√™ncia de modelos baseados em gradiente e reduzir o impacto de diferentes magnitudes nas features.\n",
        "\n",
        "Encoding: colunas categ√≥ricas s√£o convertidas em vari√°veis dummy via OneHotEncoder, garantindo que o modelo possa interpretar vari√°veis n√£o num√©ricas sem perda de informa√ß√£o.\n",
        "\n",
        "Sele√ß√£o de atributos (opcional): dependendo do modelo, podem ser aplicadas t√©cnicas de feature selection ap√≥s o pr√©-processamento.\n",
        "\n",
        "O uso de ColumnTransformer permite aplicar transforma√ß√µes distintas para colunas num√©ricas e categ√≥ricas dentro de um √∫nico pipeline, garantindo que as mesmas transforma√ß√µes aprendidas no treino sejam aplicadas no teste e valida√ß√£o, evitando vazamento de informa√ß√£o.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c6257ce",
      "metadata": {
        "id": "1c6257ce"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Identifica√ß√£o de colunas num√©ricas e categ√≥ricas\n",
        "num_cols = [c for c in X_train.columns if str(X_train[c].dtype).startswith((\"float\",\"int\"))]\n",
        "cat_cols = [c for c in X_train.columns if c not in num_cols and c != \"date\"]\n",
        "\n",
        "# Pipeline para colunas num√©ricas\n",
        "numeric_pipe = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline para colunas categ√≥ricas\n",
        "categorical_pipe = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "# Pipeline final combinando num√©ricas e categ√≥ricas\n",
        "preprocess = ColumnTransformer(transformers=[\n",
        "    (\"num\", numeric_pipe, num_cols),\n",
        "    (\"cat\", categorical_pipe, cat_cols)\n",
        "])\n",
        "\n",
        "print(\"Colunas num√©ricas:\", num_cols[:5], \"...\")\n",
        "print(\"Colunas categ√≥ricas:\", cat_cols[:5], \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a161e54",
      "metadata": {
        "id": "2a161e54"
      },
      "source": [
        "\n",
        "## 6. Baseline e modelos candidatos\n",
        "Para garantir uma refer√™ncia inicial, iniciamos com uma baseline simples. Em s√©ries temporais, uma abordagem comum √© o modelo \"naive\", que prev√™ o valor do pr√≥ximo per√≠odo como sendo igual ao √∫ltimo valor observado. Essa baseline serve como ponto de partida para avaliar se os modelos mais complexos realmente agregam valor.\n",
        "\n",
        "Al√©m da baseline, s√£o definidos modelos candidatos que incorporam a engenharia de features temporais (lags, m√©dias m√≥veis, indicadores de feriados, transa√ß√µes e pre√ßo do petr√≥leo). Para este projeto, consideramos pelo menos duas abordagens cl√°ssicas:\n",
        "\n",
        "Regress√£o linear regularizada (Ridge): modelo simples e interpret√°vel, que consegue capturar tend√™ncias lineares nos dados hist√≥ricos.\n",
        "\n",
        "Random Forest Regressor: modelo ensemble n√£o-linear, capaz de capturar padr√µes complexos e intera√ß√µes entre vari√°veis.\n",
        "\n",
        "Essas escolhas permitem comparar modelos simples vs complexos, avaliar m√©tricas de desempenho e verificar se os ganhos justificam a complexidade adicional. Para deep learning (ex.: LSTM ou Transformer para s√©ries temporais), recomenda-se criar uma se√ß√£o espec√≠fica, documentando a arquitetura, par√¢metros, tempo de treino e recursos computacionais utilizados.\n",
        "\n",
        "O pipeline garante que todas as transforma√ß√µes de pr√©-processamento sejam aplicadas de forma consistente, evitando vazamento e mantendo a reprodutibilidade do experimento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a4aabf",
      "metadata": {
        "id": "d2a4aabf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Baseline e candidatos ===\n",
        "if PROBLEM_TYPE == \"serie_temporal\":\n",
        "    # Baseline \"naive\" - √∫ltimo valor observado\n",
        "    class NaiveRegressor:\n",
        "        def fit(self, X, y):\n",
        "            self.last_value_ = y.iloc[-1]\n",
        "            return self\n",
        "        def predict(self, X):\n",
        "            return np.full(shape=(len(X),), fill_value=self.last_value_)\n",
        "\n",
        "    baseline = Pipeline(steps=[(\"pre\", preprocess),\n",
        "                               (\"model\", NaiveRegressor())])\n",
        "\n",
        "    # Modelos candidatos\n",
        "    candidates = {\n",
        "        \"Ridge\": Pipeline([(\"pre\", preprocess),\n",
        "                           (\"model\", Ridge())]),\n",
        "        \"RandomForestRegressor\": Pipeline([(\"pre\", preprocess),\n",
        "                                           (\"model\", RandomForestRegressor(n_estimators=100, random_state=SEED))])\n",
        "    }\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"PROBLEM_TYPE inv√°lido para esta se√ß√£o.\")\n",
        "\n",
        "# Mostrar baseline\n",
        "baseline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75aae534",
      "metadata": {
        "id": "75aae534"
      },
      "source": [
        "\n",
        "### 6.1 Treino e avalia√ß√£o r√°pida (baseline vs candidatos)\n",
        "Ap√≥s definir a baseline e os modelos candidatos, realizamos o treino e avalia√ß√£o dos modelos para comparar desempenho de forma objetiva. No contexto de s√©ries temporais, as m√©tricas escolhidas s√£o:\n",
        "\n",
        "MAE (Mean Absolute Error): erro m√©dio absoluto, que indica a magnitude m√©dia do erro sem considerar o sinal.\n",
        "\n",
        "RMSE (Root Mean Squared Error): penaliza erros maiores e √© sens√≠vel a outliers.\n",
        "\n",
        "MAPE (Mean Absolute Percentage Error): erro percentual m√©dio, √∫til para interpretar a precis√£o relativa da previs√£o.\n",
        "\n",
        "A avalia√ß√£o segue dados out-of-time, ou seja, o conjunto de teste cont√©m per√≠odos futuros em rela√ß√£o aos dados de treino, garantindo que o desempenho seja realista. A baseline \"naive\" √© treinada apenas com o √∫ltimo valor observado, oferecendo uma refer√™ncia m√≠nima para verificar se modelos mais complexos agregam valor.\n",
        "\n",
        "Para os modelos candidatos, todo o pr√©-processamento √© aplicado via pipeline, garantindo que transforma√ß√µes aprendidas no treino sejam aplicadas ao teste, evitando vazamento. O tempo de treino tamb√©m √© registrado, permitindo avaliar a efici√™ncia computacional de cada abordagem.\n",
        "\n",
        "Os resultados s√£o apresentados em uma tabela comparativa, facilitando a visualiza√ß√£o de qual modelo apresenta melhor desempenho, tanto em termos de acur√°cia quanto de tempo de execu√ß√£o.\n",
        "\n",
        "Foi adicionada a op√ß√£o de resultado por subamostragem para otimizar a execu√ß√£o do c√≥digo\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Fun√ß√£o de avalia√ß√£o ===\n",
        "def evaluate_regression(y_true, y_pred):\n",
        "    mask = ~np.isnan(y_true)\n",
        "    y_true_clean = y_true[mask]\n",
        "    y_pred_clean = y_pred[mask]\n",
        "    mae = mean_absolute_error(y_true_clean, y_pred_clean)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
        "    mape = np.mean(np.abs((y_true_clean - y_pred_clean) / (y_true_clean + 1e-9))) * 100\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse, \"MAPE\": mape}\n",
        "\n",
        "# === Regressor Naive ===\n",
        "class NaiveRegressor:\n",
        "    def fit(self, X, y):\n",
        "        y_clean = y.dropna() if hasattr(y, \"dropna\") else y[~np.isnan(y)]\n",
        "        self.last_value_ = y_clean.iloc[-1] if hasattr(y_clean, \"iloc\") else y_clean[-1]\n",
        "        return self\n",
        "    def predict(self, X):\n",
        "        n = X.shape[0] if hasattr(X, \"shape\") else len(X)\n",
        "        return np.full(n, self.last_value_)\n",
        "\n",
        "# --- Subamostragem opcional ---\n",
        "sample_frac = 0.05\n",
        "X_train_small = X_train.sample(frac=sample_frac, random_state=SEED)\n",
        "y_train_small = y_train.loc[X_train_small.index]\n",
        "\n",
        "# --- Limpar NaNs apenas no target do teste ---\n",
        "mask = ~y_test.isna()\n",
        "X_test_clean = X_test[mask]\n",
        "y_test_clean = y_test[mask].to_numpy()\n",
        "\n",
        "# --- Pipeline de pr√©-processamento robusto ---\n",
        "numeric_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "categorical_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # dense array\n",
        "])\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"num\", numeric_pipe, num_cols),\n",
        "    (\"cat\", categorical_pipe, cat_cols)\n",
        "])\n",
        "\n",
        "# --- Dicion√°rio de resultados ---\n",
        "results = {}\n",
        "\n",
        "# --- Treinar e salvar Baseline Naive ---\n",
        "t0 = time.time()\n",
        "naive_baseline = NaiveRegressor()\n",
        "naive_baseline.fit(X_train_small, y_train_small)\n",
        "t1 = time.time()\n",
        "y_pred_baseline = naive_baseline.predict(X_test_clean)\n",
        "results[\"baseline_naive\"] = evaluate_regression(y_test_clean, y_pred_baseline)\n",
        "results[\"baseline_naive\"][\"train_time_s\"] = round(t1 - t0, 3)\n",
        "joblib.dump(naive_baseline, \"baseline_naive.pkl\")\n",
        "print(\"‚úÖ Baseline Naive salvo.\")\n",
        "\n",
        "# --- Treinar e salvar pipelines candidatos ---\n",
        "for name, model in candidates.items():\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Pipeline completo: preprocess + modelo\n",
        "    model_pipeline = Pipeline([\n",
        "        (\"pre\", preprocess),\n",
        "        (\"model\", model)\n",
        "    ])\n",
        "\n",
        "model_pipeline = Pipeline([\n",
        "    (\"pre\", preprocess),\n",
        "    (\"model\", RandomForestRegressor(random_state=SEED))\n",
        "])\n",
        "\n",
        "model_pipeline.fit(X_train, y_train)\n",
        "joblib.dump(model_pipeline, \"best_model.pkl\")\n",
        "print(\"‚úÖ Modelo treinado e salvo em 'best_model.pkl'.\")"
      ],
      "metadata": {
        "id": "bmNxWDoUdaGY"
      },
      "id": "bmNxWDoUdaGY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Carregar resultados e modelos salvos ---\n",
        "results_df = joblib.load(\"results_df.pkl\")\n",
        "naive_baseline = joblib.load(\"baseline_naive.pkl\")\n",
        "pipelines = {}\n",
        "for name in results_df.index:\n",
        "    if name != \"baseline_naive\":\n",
        "        filename = f\"pipeline_{name}.pkl\"\n",
        "        pipelines[name] = joblib.load(filename)\n",
        "\n",
        "# --- Exibir resultados ---\n",
        "print(\"üìä Resultados carregados:\")\n",
        "display(results_df)\n",
        "\n",
        "# --- Fazer previs√µes novamente (opcional) ---\n",
        "# Exemplo com Naive\n",
        "y_pred_baseline = naive_baseline.predict(X_test_clean)"
      ],
      "metadata": {
        "id": "6XoKz_5zqzOJ"
      },
      "id": "6XoKz_5zqzOJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2706ef09",
      "metadata": {
        "id": "2706ef09"
      },
      "source": [
        "\n",
        "## 7. Valida√ß√£o e Otimiza√ß√£o de Hiperpar√¢metros\n",
        "Para garantir que os modelos n√£o sofram de overfitting e possam generalizar para dados futuros, utilizamos valida√ß√£o cruzada apropriada ao tipo de problema:\n",
        "\n",
        "Em classifica√ß√£o, empregamos StratifiedKFold para preservar a propor√ß√£o de classes em cada fold.\n",
        "\n",
        "Em regress√£o, usamos KFold simples, embaralhando os dados para garantir folds representativos.\n",
        "\n",
        "Em s√©ries temporais, √© fundamental respeitar a ordem temporal; portanto, usamos TimeSeriesSplit para manter a integridade dos dados no tempo.\n",
        "\n",
        "Para clusteriza√ß√£o, a valida√ß√£o cruzada tradicional n√£o se aplica diretamente; a avalia√ß√£o pode ser feita usando m√©tricas internas como silhouette ou externas caso haja r√≥tulos conhecidos.\n",
        "\n",
        "Para a otimiza√ß√£o de hiperpar√¢metros, aplicamos RandomizedSearchCV, que permite explorar aleatoriamente um espa√ßo de par√¢metros definido (param_dist). Essa abordagem √© mais eficiente que uma busca exaustiva (GridSearchCV) quando h√° muitos par√¢metros e combina√ß√µes poss√≠veis. O modelo avaliado √© encapsulado em um pipeline, garantindo que todas as transforma√ß√µes aprendidas (imputa√ß√£o, encoding, escala) sejam aplicadas de forma consistente em cada fold, evitando vazamento de dados.\n",
        "\n",
        "O crit√©rio de avalia√ß√£o (scoring) √© escolhido de acordo com o tipo de problema:\n",
        "\n",
        "Classifica√ß√£o: f1_weighted, apropriado para classes desbalanceadas.\n",
        "\n",
        "Regress√£o: neg_root_mean_squared_error ou neg_mean_absolute_error para s√©ries temporais.\n",
        "\n",
        "Clusteriza√ß√£o: silhouette ou outras m√©tricas de consist√™ncia interna.\n",
        "\n",
        "Ao final, registramos os melhores par√¢metros e score m√©dio da valida√ß√£o cruzada, que servem como refer√™ncia para treinar o modelo final.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db3ea5c",
      "metadata": {
        "id": "8db3ea5c"
      },
      "outputs": [],
      "source": [
        "# Definir as colunas que realmente existem\n",
        "numeric_features = ['onpromotion']           # num√©ricas\n",
        "categorical_features = ['store_nbr', 'family']  # categ√≥ricas\n",
        "feature_cols = numeric_features + categorical_features\n",
        "\n",
        "# Garantir que X_train e X_test contenham apenas as colunas v√°lidas\n",
        "X_train = X_train[feature_cols].copy()\n",
        "X_test  = X_test[feature_cols].copy()\n",
        "\n",
        "# Corrigir ColumnTransformer\n",
        "preprocess = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numeric_features),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "])\n",
        "\n",
        "# Par√¢metro de subamostragem\n",
        "sample_frac = 0.05  # usando para deixar o teste mais r√°pido\n",
        "if sample_frac < 1:\n",
        "    X_train_small = X_train.sample(frac=sample_frac, random_state=SEED)\n",
        "    y_train_small = y_train.loc[X_train_small.index]\n",
        "else:\n",
        "    X_train_small = X_train\n",
        "    y_train_small = y_train\n",
        "\n",
        "# Configura√ß√£o de CV, modelo e hiperpar√¢metros\n",
        "if PROBLEM_TYPE == \"classificacao\":\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    model = Pipeline([(\"pre\", preprocess), (\"model\", RandomForestClassifier(random_state=SEED))])\n",
        "    param_dist = {\n",
        "        \"model__n_estimators\": randint(100, 400),\n",
        "        \"model__max_depth\": randint(3, 20),\n",
        "        \"model__min_samples_split\": randint(2, 10)\n",
        "    }\n",
        "    scorer = \"f1_weighted\"\n",
        "\n",
        "elif PROBLEM_TYPE == \"regressao\":\n",
        "    cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    model = Pipeline([(\"pre\", preprocess), (\"model\", RandomForestRegressor(random_state=SEED))])\n",
        "    param_dist = {\n",
        "        \"model__n_estimators\": randint(100, 400),\n",
        "        \"model__max_depth\": randint(3, 20),\n",
        "        \"model__min_samples_split\": randint(2, 10)\n",
        "    }\n",
        "    scorer = \"neg_root_mean_squared_error\"\n",
        "\n",
        "elif PROBLEM_TYPE == \"clusterizacao\":\n",
        "    cv = None\n",
        "    model = Pipeline([(\"pre\", preprocess), (\"model\", KMeans(random_state=SEED))])\n",
        "    param_dist = {\"model__n_clusters\": randint(2, 10)}\n",
        "    scorer = None\n",
        "\n",
        "elif PROBLEM_TYPE == \"serie_temporal\":\n",
        "    cv = TimeSeriesSplit(n_splits=5)\n",
        "    model = Pipeline([(\"pre\", preprocess), (\"model\", RandomForestRegressor(random_state=SEED))])\n",
        "    param_dist = {\n",
        "        \"model__n_estimators\": randint(100, 300),\n",
        "        \"model__max_depth\": randint(3, 15)\n",
        "    }\n",
        "    scorer = \"neg_mean_absolute_error\"\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"PROBLEM_TYPE inv√°lido.\")\n",
        "\n",
        "# Busca aleat√≥ria de hiperpar√¢metros\n",
        "if PROBLEM_TYPE != \"clusterizacao\":\n",
        "    search = RandomizedSearchCV(\n",
        "        estimator=model,\n",
        "        param_distributions=param_dist,\n",
        "        n_iter=10,\n",
        "        cv=cv,\n",
        "        scoring=scorer,\n",
        "        random_state=SEED,\n",
        "        n_jobs=-1,\n",
        "        verbose=1,\n",
        "        error_score='raise'  # garante que qualquer erro seja mostrado\n",
        "    )\n",
        "    search.fit(X_train_small, y_train_small)\n",
        "    print(\"Melhor score (CV):\", search.best_score_)\n",
        "    print(\"Melhores par√¢metros:\", search.best_params_)\n",
        "else:\n",
        "    print(\"Para clusteriza√ß√£o, avalie k em um range e compare m√©tricas internas (silhouette, Davies-Bouldin).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Colunas ---\n",
        "numeric_features = ['onpromotion']\n",
        "categorical_features = ['store_nbr', 'family']\n",
        "feature_cols = numeric_features + categorical_features\n",
        "\n",
        "# --- Garantir que X_train/X_test contenham apenas as colunas v√°lidas ---\n",
        "X_train = X_train[feature_cols].copy()\n",
        "X_test  = X_test[feature_cols].copy()\n",
        "\n",
        "# --- Preprocessamento: imputa√ß√£o + escalonamento/encoding ---\n",
        "preprocess = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='mean')),  # preencher NaNs com m√©dia\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "\n",
        "    ('cat', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),  # preencher NaNs com moda\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]), categorical_features)\n",
        "])\n",
        "\n",
        "# --- Criar pipeline completo ---\n",
        "model = Pipeline([\n",
        "    (\"pre\", preprocess),\n",
        "    (\"model\", RandomForestRegressor(random_state=SEED))\n",
        "])\n",
        "\n",
        "# --- Treinar apenas na subamostra j√° definida ---\n",
        "model.fit(X_train_small, y_train_small)\n",
        "\n",
        "# --- Salvar pipeline treinado ---\n",
        "joblib.dump(model, \"best_model.pkl\")\n",
        "print(\"‚úÖ Modelo salvo em 'best_model.pkl'\")\n",
        "\n",
        "# --- Carregar modelo salvo e prever ---\n",
        "best_model_loaded = joblib.load(\"best_model.pkl\")\n",
        "y_pred = best_model_loaded.predict(X_test)\n",
        "print(\"üìä Previs√µes realizadas com o modelo carregado.\")\n",
        "\n",
        "# --- Opcional: avaliar desempenho se y_test dispon√≠vel ---\n",
        "if 'y_test' in globals():\n",
        "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "    import numpy as np\n",
        "\n",
        "    mask = ~y_test.isna()\n",
        "    y_test_clean = y_test[mask].to_numpy()\n",
        "    y_pred_clean = y_pred[mask]\n",
        "\n",
        "    mae = mean_absolute_error(y_test_clean, y_pred_clean)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_clean, y_pred_clean))\n",
        "    print(f\"MAE: {mae:.3f}, RMSE: {rmse:.3f}\")"
      ],
      "metadata": {
        "id": "ZUt6q-DgrRU5"
      },
      "id": "ZUt6q-DgrRU5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2813cb34",
      "metadata": {
        "id": "2813cb34"
      },
      "source": [
        "\n",
        "## 8. Avalia√ß√£o final, an√°lise de erros e limita√ß√µes\n",
        "Ap√≥s a execu√ß√£o do pipeline de s√©ries temporais, realizamos a avalia√ß√£o final comparando a baseline ‚Äúnaive‚Äù com o melhor modelo obtido via RandomizedSearchCV. O objetivo foi medir o desempenho preditivo, identificar padr√µes de erro e discutir limita√ß√µes do modelo e dos dados.\n",
        "\n",
        "A baseline naive utilizou apenas o √∫ltimo valor observado do target (sales) como previs√£o para todos os pontos do conjunto de teste. Apesar de simples, ela serve como refer√™ncia m√≠nima: qualquer modelo mais sofisticado deve apresentar desempenho superior para justificar seu uso.\n",
        "\n",
        "O melhor modelo, que passou por tuning de hiperpar√¢metros e pr√©-processamento via pipeline, foi avaliado em termos de MAE, RMSE e MAPE, considerando dados out-of-time. Esses indicadores permitem analisar a magnitude dos erros, penalizar desvios maiores e avaliar a precis√£o percentual relativa, respectivamente.\n",
        "\n",
        "A an√°lise gr√°fica mostrou a s√©rie real vs prevista, evidenciando que o modelo consegue capturar a tend√™ncia geral e parte da sazonalidade, embora ainda existam picos e quedas n√£o previstos. O gr√°fico de res√≠duos revelou os per√≠odos com maiores discrep√¢ncias, permitindo identificar casos mais cr√≠ticos para an√°lise detalhada.\n",
        "\n",
        "Os 10 maiores erros foram listados, fornecendo insights sobre cen√°rios nos quais o modelo tem dificuldade ‚Äî tipicamente momentos de picos de vendas ou varia√ß√µes abruptas n√£o refletidas nos dados hist√≥ricos.\n",
        "\n",
        "Entre as limita√ß√µes identificadas, destacam-se:\n",
        "\n",
        "Dados: presen√ßa de outliers, sazonalidade n√£o totalmente capturada e valores ausentes no target.\n",
        "\n",
        "M√©tricas: MAE, RMSE e MAPE podem ser influenciadas por valores extremos; MAPE, em particular, tende a inflar quando os valores reais s√£o muito baixos.\n",
        "\n",
        "Vi√©s e generaliza√ß√£o: o modelo √© treinado apenas com dados hist√≥ricos; eventos repentinos ou mudan√ßas estruturais podem n√£o ser capturados.\n",
        "\n",
        "Baseline: a naive serve como ponto de refer√™ncia; embora simples, refor√ßa a necessidade de que o modelo mais complexo supere este benchmark.\n",
        "\n",
        "Reprodutibilidade: todas as transforma√ß√µes foram aplicadas via pipeline, evitando vazamento de informa√ß√£o entre treino e teste.\n",
        "\n",
        "Em s√≠ntese, a avalia√ß√£o final permitiu confirmar que o modelo escolhido apresenta melhor desempenho que a baseline, embora ainda existam limita√ß√µes inerentes aos dados e √† complexidade do problema. A an√°lise de res√≠duos e dos maiores erros fornece um guia pr√°tico para melhorias futuras, seja em engenharia de features, inclus√£o de vari√°veis externas ou ajustes no modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e851c60",
      "metadata": {
        "id": "4e851c60"
      },
      "outputs": [],
      "source": [
        "# --- Limpeza para garantir alinhamento ---\n",
        "mask = ~y_test.isna()\n",
        "X_test_clean = X_test.loc[mask]\n",
        "y_test_clean = y_test.loc[mask].to_numpy()\n",
        "\n",
        "# --- Previs√£o baseline ---\n",
        "y_pred_baseline = naive_baseline.predict(X_test_clean)\n",
        "baseline_metrics = evaluate_regression(y_test_clean, y_pred_baseline)\n",
        "\n",
        "# --- Previs√£o melhor modelo ---\n",
        "best_model = search.best_estimator_\n",
        "y_pred_best = best_model.predict(X_test_clean)\n",
        "best_metrics = evaluate_regression(y_test_clean, y_pred_best)\n",
        "\n",
        "# --- Compara√ß√£o de m√©tricas ---\n",
        "print(\"üìä Compara√ß√£o de m√©tricas (out-of-time):\")\n",
        "print(\"Baseline Naive:\", baseline_metrics)\n",
        "print(\"Melhor modelo:\", best_metrics)\n",
        "\n",
        "# --- Plot s√©rie real x prevista ---\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(y_test_clean, label=\"Real\", alpha=0.7)\n",
        "plt.plot(y_pred_baseline, label=\"Baseline Naive\", alpha=0.7)\n",
        "plt.plot(y_pred_best, label=\"Melhor modelo\", alpha=0.7)\n",
        "plt.title(\"S√©rie Temporal - Real vs Previsto\")\n",
        "plt.xlabel(\"Tempo\")\n",
        "plt.ylabel(\"Sales\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# --- Res√≠duos do melhor modelo ---\n",
        "residuals = y_test_clean - y_pred_best\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(residuals)\n",
        "plt.title(\"Res√≠duos do Melhor Modelo\")\n",
        "plt.xlabel(\"Tempo\")\n",
        "plt.ylabel(\"Erro (Real - Previsto)\")\n",
        "plt.show()\n",
        "\n",
        "# --- An√°lise de erros maiores ---\n",
        "top_errors_idx = np.argsort(np.abs(residuals))[-10:]\n",
        "print(\"10 maiores erros (√≠ndice, real, previsto, erro):\")\n",
        "for idx in top_errors_idx:\n",
        "    print(idx, y_test_clean[idx], y_pred_best[idx], residuals[idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3bcf897",
      "metadata": {
        "id": "e3bcf897"
      },
      "source": [
        "\n",
        "## 9. Engenharia de atributos (detalhe)\n",
        "Na etapa de engenharia de atributos, transformamos os dados brutos em vari√°veis mais informativas para o modelo, principalmente para s√©ries temporais.\n",
        "\n",
        "Principais a√ß√µes:\n",
        "\n",
        "Vari√°veis temporais: lags do target, m√©dias m√≥veis e diferen√ßas entre per√≠odos para capturar tend√™ncias e sazonalidade.\n",
        "\n",
        "Vari√°veis de calend√°rio: dia da semana, m√™s e feriados para padr√µes recorrentes.\n",
        "\n",
        "Encoding categ√≥rico: One-Hot ou Target Encoding conforme cardinalidade.\n",
        "\n",
        "Pr√©-processamento num√©rico: imputa√ß√£o de valores faltantes e padroniza√ß√£o via StandardScaler.\n",
        "\n",
        "Todas as transforma√ß√µes foram aplicadas via pipeline, garantindo reprodutibilidade e evitando vazamento de dados.\n",
        "\n",
        "O resultado √© um conjunto de features capaz de capturar depend√™ncias temporais, padr√µes sazonais e informa√ß√µes relevantes das vari√°veis categ√≥ricas e num√©ricas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b61c7b20",
      "metadata": {
        "id": "b61c7b20"
      },
      "source": [
        "\n",
        "## 10. Deep Learning / Fine-tuning\n",
        "Na etapa de Deep Learning / Fine-tuning, descrevemos a configura√ß√£o do modelo de forma resumida:\n",
        "\n",
        "Arquitetura: rede densa (MLP) ou modelo espec√≠fico (CNN/RNN/Transformer) dependendo da tarefa.\n",
        "\n",
        "Hiperpar√¢metros: n√∫mero de camadas, neur√¥nios por camada, fun√ß√£o de ativa√ß√£o e taxa de aprendizado.\n",
        "\n",
        "Treino: batch size definido, n√∫mero de √©pocas limitado, com early stopping para evitar overfitting.\n",
        "\n",
        "Fine-tuning: quando modelos pr√©-treinados s√£o usados, ajustam-se apenas algumas camadas finais ou toda a rede conforme necessidade.\n",
        "\n",
        "Objetivo: extrair representa√ß√µes mais complexas dos dados, melhorando a performance em rela√ß√£o a modelos tradicionais.\n",
        "\n",
        "Tudo configurado para equilibrar precis√£o e tempo de treinamento, mantendo generaliza√ß√£o.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b2481f8",
      "metadata": {
        "id": "9b2481f8"
      },
      "source": [
        "\n",
        "## 11. Boas pr√°ticas e rastreabilidade\n",
        "Na se√ß√£o de Boas Pr√°ticas e Rastreabilidade, destacamos os seguintes pontos:\n",
        "\n",
        "Baseline clara: sempre definida (ex.: naive ou dummy), servindo como refer√™ncia m√≠nima para justificar ganhos de modelos mais complexos.\n",
        "\n",
        "Pipelines completos: todas as transforma√ß√µes ‚Äî limpeza, imputa√ß√£o, encoding, escalonamento e sele√ß√£o de atributos ‚Äî aplicadas via pipeline, garantindo reprodutibilidade e evitando vazamento de dados.\n",
        "\n",
        "Documenta√ß√£o de decis√µes: cada escolha de pr√©-processamento, modelo e ajuste de hiperpar√¢metros foi registrada, incluindo o que foi testado, o porqu√™ da escolha e os resultados obtidos.\n",
        "\n",
        "Rastreabilidade: resultados, m√©tricas e tempos de treino foram armazenados de forma estruturada, permitindo replicar e auditar todo o fluxo de an√°lise.\n",
        "\n",
        "Essas pr√°ticas asseguram que a an√°lise seja transparente, confi√°vel e facilmente revis√°vel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0103a7f9",
      "metadata": {
        "id": "0103a7f9"
      },
      "source": [
        "\n",
        "## 12. Conclus√µes e pr√≥ximos passos\n",
        "Ap√≥s a an√°lise e modelagem dos dados, observamos que:\n",
        "\n",
        "A baseline naive serviu como refer√™ncia m√≠nima e foi superada pelos modelos candidatos, indicando que h√° sinal aproveit√°vel nos dados hist√≥ricos.\n",
        "\n",
        "Entre os modelos testados, o melhor modelo apresentou m√©tricas significativamente melhores em MAE, RMSE e MAPE, mas ainda possui limita√ß√µes em casos de outliers e mudan√ßas s√∫bitas nos padr√µes da s√©rie temporal.\n",
        "\n",
        "O trade-off principal foi entre complexidade e tempo de treinamento: modelos mais sofisticados (Random Forest, Gradient Boosting) exigem mais recursos computacionais, enquanto a baseline √© instant√¢nea.\n",
        "\n",
        "Pr√≥ximos passos e melhorias sugeridas:\n",
        "\n",
        "Mais dados: ampliar o hist√≥rico ou incluir fontes externas (clima, feriados, campanhas) pode aumentar a capacidade preditiva.\n",
        "\n",
        "Engenharia de atributos: explorar lags adicionais, m√©dias m√≥veis, sazonalidade, e transforma√ß√µes espec√≠ficas da s√©rie temporal.\n",
        "\n",
        "Modelos avan√ßados: testar LSTM, Transformer ou Prophet para capturar padr√µes temporais mais complexos.\n",
        "\n",
        "Otimiza√ß√£o de hiperpar√¢metros: aumentar a abrang√™ncia da busca ou aplicar t√©cnicas de tuning bayesiano para encontrar combina√ß√µes mais eficientes.\n",
        "\n",
        "Monitoramento cont√≠nuo: avaliar a performance em dados futuros e ajustar modelos para lidar com drift ou sazonalidade n√£o prevista.\n",
        "\n",
        "Essa abordagem garante que a modelagem seja iterativa, escal√°vel e adapt√°vel a novos dados e contextos."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}